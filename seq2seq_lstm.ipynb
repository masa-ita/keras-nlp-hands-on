{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QetvaJ27Tb6y",
        "outputId": "ef333356-05b9-42b6-aa09-491d01bc3859"
      },
      "source": [
        "!pip install janome japanize_matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting japanize_matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/85/08a4b7fe8987582d99d9bb7ad0ff1ec75439359a7f9690a0dbf2dbf98b15/japanize-matplotlib-1.1.3.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->japanize_matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-cp36-none-any.whl size=4120276 sha256=38baf20d9243bd3a212e4d006d49251214730da161e32f35cce22debba571ed0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/d9/a2/f907d50b32a2d2008ce5d691d30fb6569c2c93eefcfde55202\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: janome, japanize-matplotlib\n",
            "Successfully installed janome-0.4.1 japanize-matplotlib-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M-eDSyL-8E7",
        "outputId": "f60666ef-bbee-46e0-d95f-40b77cb6bb1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "gz_file_path = get_file('examples_pd.gz', 'ftp://ftp.monash.edu/pub/nihongo/examples_pd.gz',)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from ftp://ftp.monash.edu/pub/nihongo/examples_pd.gz\n",
            "8388608/8386099 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsrpAmZ1BVFF"
      },
      "source": [
        "import gzip\n",
        "import pandas as pd\n",
        "\n",
        "with open(gz_file_path, 'rb') as fd:\n",
        "    gzip_fd = gzip.GzipFile(fileobj=fd)\n",
        "    raw_lines = gzip_fd.readlines()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8meT2QYBrTN",
        "outputId": "d9e7790a-5c75-4beb-e004-ae1ae06e8cfe"
      },
      "source": [
        "import re\n",
        "\n",
        "NUM_EXAMPLES = 30000\n",
        "\n",
        "re_line = re.compile(r'(A|B):\\s(.+)\\t(.+)(?:#ID=\\d+\\r\\n)')\n",
        "\n",
        "en = []\n",
        "ja = []\n",
        "\n",
        "for raw_line in raw_lines[:NUM_EXAMPLES]:\n",
        "    raw_line = raw_line.decode('euc_jisx0213')\n",
        "    m = re_line.match(raw_line)\n",
        "    if m and m[1] == 'A':\n",
        "        ja.append(m[2])\n",
        "        en.append(m[3])\n",
        "print(ja[:5])\n",
        "print(en[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['＆という記号は、ａｎｄを指す。', '＆のマークはａｎｄの文字を表す。', '（自転車に乗って）フーッ、この坂道はきついよ。でも帰りは楽だよね。', '実のところ物価は毎週上昇している。', '〜と痛切に感じている。']\n",
            "[\"The sign '&' stands for 'and'.\", 'The mark \"&\" stands for \"and\".', '(On a bicycle) Whew! This is a tough hill. But coming back sure will be a breeze.', 'As it is, prices are going up every week.', 'I was acutely aware that..']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNHMSVjG4_C"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_en(s):\n",
        "    return unicodedata.normalize('NFD', s)\n",
        "\n",
        "def normalize_ja(s):\n",
        "    return unicodedata.normalize('NFKC', s)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Knl8HkaSUvM"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "t_wakati = Tokenizer(wakati=True)\n",
        "\n",
        "def tokenize_japanese(text):\n",
        "    return ' '.join(list(t_wakati.tokenize(text)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drPlWcVMMj6d"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_en(w):\n",
        "    w = normalize_en(w.lower().strip())\n",
        "\n",
        "    # 単語とそのあとの句読点の間にスペースを挿入\n",
        "    # 例：　\"he is a boy.\" => \"he is a boy .\"\n",
        "    # 参照：- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # 文の開始と終了のトークンを付加\n",
        "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "def preprocess_ja(w):\n",
        "    w = normalize_ja(w)\n",
        "\n",
        "    w = tokenize_japanese(w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # 文の開始と終了のトークンを付加\n",
        "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVVheh_zNpss",
        "outputId": "4e7db2fe-5e61-49ae-8241-e12a9b940a05"
      },
      "source": [
        "en_example = \"The sign '&' stands for 'and'.\"\n",
        "ja_example = '＆という記号は、ａｎｄを指す。'\n",
        "print(preprocess_en(en_example))\n",
        "print(preprocess_ja(ja_example))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> the sign '&' stands for 'and' . <end>\n",
            "<start> & という 記号 は 、 and を 指す 。 <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqAiXQJ3TZGs",
        "outputId": "1c5d36d8-5ddd-44d2-96f1-038eb6c6a90c"
      },
      "source": [
        "en = [preprocess_en(s) for s in en]\n",
        "ja = [preprocess_ja(s) for s in ja]\n",
        "print(en[-1])\n",
        "print(ja[-1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> this toothbrush is not used by my mother . <end>\n",
            "<start> この 歯ブラシ を 使っ て いる の は 母 で は ない 。 <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf4fwROcVrVP"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                           padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyvASA-0W3f0"
      },
      "source": [
        "def create_dataset(targ_lang, inp_lang):\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmmoXQBwXO_R"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = create_dataset(ja, en)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGr90EQXZDe"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-HKiv5iXnke"
      },
      "source": [
        "# ターゲットテンソルの最大長を計算\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nT12tjPXtqT",
        "outputId": "6a9d20d0-4942-44b4-f204-7bd0ae8e2b5d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80-20で分割を行い、訓練用と検証用のデータセットを作成\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# 長さを表示\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12000 12000 3000 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxSreRrXvw4"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBMUz4Q9Xxe7",
        "outputId": "a7a60390-fc9e-413e-ed8d-5c039907a8b4"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "36 ----> how\n",
            "57 ----> did\n",
            "4 ----> you\n",
            "55 ----> come\n",
            "33 ----> here\n",
            "9 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "11 ----> あなた\n",
            "4 ----> は\n",
            "929 ----> どういう\n",
            "739 ----> 風\n",
            "6 ----> に\n",
            "16 ----> し\n",
            "8 ----> て\n",
            "37 ----> ここ\n",
            "65 ----> へ\n",
            "88 ----> 来\n",
            "42 ----> まし\n",
            "10 ----> た\n",
            "13 ----> か\n",
            "3 ----> 。\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYSOGLzYbP8"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkZvi19gYp03",
        "outputId": "babaac55-41d8-4e34-f33c-e594a72361a7"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 47]), TensorShape([64, 66]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYHYgc4hYwly"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=False,\n",
        "                                       return_state=True)\n",
        " \n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, *states = self.lstm(x)\n",
        "        return output, states\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5fs_ECmZP8N",
        "outputId": "82de3f5d-72ed-4753-c327-8171d585b809"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
        "\n",
        "# サンプル入力\n",
        "sample_output, sample_hidden = encoder(example_input_batch)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
        "print ('Encoder Carry state shape: (batch size, units) {}'.format(sample_hidden[1].shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Encoder Carry state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuNjW0lZs1y"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Embeddingの出力と、エンコーダ出力を LSTM 層に渡す\n",
        "        output, *states = self.lstm(x, initial_state=hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, states"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP8fSgqDZ9Ci",
        "outputId": "2c29dadb-49ed-4640-8473-f40c20090e05"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
        "\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 10804)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOmW4oGQaBuJ"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                             from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1xOnT6oaOCY"
      },
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaknH7tgaTEt"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher Forcing - 正解値を次の入力として供給\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Teacher Forcing を使用\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bNwa8eCa0Ru",
        "outputId": "fb8813c2-489c-4220-f11c-bd314cff5b29"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9174\n",
            "Epoch 1 Batch 100 Loss 1.0019\n",
            "Epoch 1 Loss 0.9926\n",
            "Time taken for 1 epoch 119.76206350326538 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7915\n",
            "Epoch 2 Batch 100 Loss 0.8475\n",
            "Epoch 2 Loss 0.7956\n",
            "Time taken for 1 epoch 60.67542600631714 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7215\n",
            "Epoch 3 Batch 100 Loss 0.6492\n",
            "Epoch 3 Loss 0.7109\n",
            "Time taken for 1 epoch 60.16311812400818 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6464\n",
            "Epoch 4 Batch 100 Loss 0.6068\n",
            "Epoch 4 Loss 0.6479\n",
            "Time taken for 1 epoch 60.796257972717285 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6283\n",
            "Epoch 5 Batch 100 Loss 0.6301\n",
            "Epoch 5 Loss 0.5927\n",
            "Time taken for 1 epoch 60.14712381362915 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5399\n",
            "Epoch 6 Batch 100 Loss 0.5475\n",
            "Epoch 6 Loss 0.5429\n",
            "Time taken for 1 epoch 60.748804330825806 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4844\n",
            "Epoch 7 Batch 100 Loss 0.4662\n",
            "Epoch 7 Loss 0.4961\n",
            "Time taken for 1 epoch 60.22471356391907 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4216\n",
            "Epoch 8 Batch 100 Loss 0.4933\n",
            "Epoch 8 Loss 0.4500\n",
            "Time taken for 1 epoch 60.73105192184448 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4170\n",
            "Epoch 9 Batch 100 Loss 0.3990\n",
            "Epoch 9 Loss 0.4055\n",
            "Time taken for 1 epoch 60.28980779647827 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3137\n",
            "Epoch 10 Batch 100 Loss 0.3120\n",
            "Epoch 10 Loss 0.3620\n",
            "Time taken for 1 epoch 60.95366644859314 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3090\n",
            "Epoch 11 Batch 100 Loss 0.3177\n",
            "Epoch 11 Loss 0.3212\n",
            "Time taken for 1 epoch 60.275187492370605 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2842\n",
            "Epoch 12 Batch 100 Loss 0.2963\n",
            "Epoch 12 Loss 0.2831\n",
            "Time taken for 1 epoch 60.92457580566406 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2642\n",
            "Epoch 13 Batch 100 Loss 0.2426\n",
            "Epoch 13 Loss 0.2481\n",
            "Time taken for 1 epoch 60.2365825176239 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1925\n",
            "Epoch 14 Batch 100 Loss 0.2023\n",
            "Epoch 14 Loss 0.2150\n",
            "Time taken for 1 epoch 60.860963344573975 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1828\n",
            "Epoch 15 Batch 100 Loss 0.1754\n",
            "Epoch 15 Loss 0.1849\n",
            "Time taken for 1 epoch 60.2426483631134 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1479\n",
            "Epoch 16 Batch 100 Loss 0.1534\n",
            "Epoch 16 Loss 0.1571\n",
            "Time taken for 1 epoch 60.75812792778015 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1030\n",
            "Epoch 17 Batch 100 Loss 0.1465\n",
            "Epoch 17 Loss 0.1314\n",
            "Time taken for 1 epoch 60.08682084083557 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1088\n",
            "Epoch 18 Batch 100 Loss 0.1045\n",
            "Epoch 18 Loss 0.1083\n",
            "Time taken for 1 epoch 60.84527254104614 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0737\n",
            "Epoch 19 Batch 100 Loss 0.0971\n",
            "Epoch 19 Loss 0.0882\n",
            "Time taken for 1 epoch 60.08582305908203 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0692\n",
            "Epoch 20 Batch 100 Loss 0.0618\n",
            "Epoch 20 Loss 0.0705\n",
            "Time taken for 1 epoch 60.93061709403992 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6kO2HY0bEn0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate(sentence):\n",
        "    sentence = preprocess_en(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input,\n",
        "                                          dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zIxy2Zf3vw"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIbzsxI-f8Z9",
        "outputId": "2d6a86bb-188f-4e7b-c972-b302bbd8e50b"
      },
      "source": [
        "# checkpoint_dir の中の最後のチェックポイントを復元\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3fe6e58e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX4evgWVf9xa",
        "outputId": "0f1ffb66-3c9c-488d-f37b-adf970865433"
      },
      "source": [
        "translate('It is necessary that the bill pass the diet.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> it is necessary that the bill pass the diet . <end>\n",
            "Predicted translation: あなた と 言い争っ て も むだ だ 。 <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhAZ9RKv9HlL"
      },
      "source": [
        "def predict(inputs):\n",
        "    inputs = tf.convert_to_tensor([inputs])\n",
        "    predicted_seq = []\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input,\n",
        "                                          dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        predicted_seq.append(predicted_id)\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return predicted_seq\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return predicted_seq\n",
        "    "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qYv1rYq1Lu5"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "input_sentences = []\n",
        "target_sentences = []\n",
        "predicted_sentences = []\n",
        "\n",
        "for i, input_en in enumerate(input_tensor_val):\n",
        "    predicted_ja = predict(input_en)\n",
        "    tokens_input = [inp_lang.index_word[id] for id in input_en if id > 2]\n",
        "    tokens_target = [targ_lang.index_word[id] for id in target_tensor_val[i] if id > 2]\n",
        "    tokens_predicted = [targ_lang.index_word[id] for id in predicted_ja if id > 2]\n",
        "    input_sentences.append(' '.join(tokens_input))\n",
        "    target_sentences.append(''.join(tokens_target))\n",
        "    predicted_sentences.append(''.join(tokens_predicted))\n",
        "\n",
        "result_df = pd.DataFrame({'input_sentence': input_sentences,\n",
        "                          'target_sentence': target_sentences,\n",
        "                          'predicted_sentence': predicted_sentences})  \n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16qn07BeMCJs"
      },
      "source": [
        "bleu_scores = []\n",
        "for row in result_df.itertuples():\n",
        "    bleu_scores.append(\n",
        "        sentence_bleu(row.target_sentence, row.predicted_sentence,\n",
        "                      smoothing_function=SmoothingFunction().method3)\n",
        "    )\n",
        "result_df['bleu_score'] = bleu_scores"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnwSEkf9HlM",
        "outputId": "c42661d2-7ea6-4dfa-bdaa-c232cda25c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "result_df"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "      <th>predicted_sentence</th>\n",
              "      <th>bleu_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>four miles is a good distance .</td>\n",
              "      <td>4マイルはかなりの距離だ。</td>\n",
              "      <td>5マイルは1ドルに等しい。</td>\n",
              "      <td>0.046192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you can make it .</td>\n",
              "      <td>あなたはそれを成し遂げることができる。</td>\n",
              "      <td>あなたは腕時計を持っています。</td>\n",
              "      <td>0.041130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am happy to see you .</td>\n",
              "      <td>あなたにお目にかかれてうれしいです。</td>\n",
              "      <td>あなたに会えてとてもうれしい。</td>\n",
              "      <td>0.046733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>out of the students , only one had read that b...</td>\n",
              "      <td>20人の生徒のうちたった一人しかその本を読んだことがなかった。</td>\n",
              "      <td>ここに一人の間にある生活が続く問題はいない。</td>\n",
              "      <td>0.029048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>that's bill all over .</td>\n",
              "      <td>いかにもビルのやりそうなことだ。</td>\n",
              "      <td>あそこには何を表すの?</td>\n",
              "      <td>0.053002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>our meeting like that is probably the kind of ...</td>\n",
              "      <td>あの人と巡り会えたのは、一期一会なのでしょうか。</td>\n",
              "      <td>うちの子供たちは、ベビーシッターの習慣にも礼儀できるように何もしらないように見える。</td>\n",
              "      <td>0.015530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>who is the boss of this company ?</td>\n",
              "      <td>この会社の社長は誰ですか。</td>\n",
              "      <td>このホテルの宿泊料金はいくらですか。</td>\n",
              "      <td>0.034934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>where on earth did you meet him ?</td>\n",
              "      <td>いったいあなたはどこで彼と会ったのですか。</td>\n",
              "      <td>いったいぜんたい彼はどこに行ったのか。</td>\n",
              "      <td>0.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>the maid was dead tired of her household chores .</td>\n",
              "      <td>お手伝いさんは毎日の家事にはほとほとうんざりしていた。</td>\n",
              "      <td>お手伝いさんは毎日の家事にすっかり飽きてしまった。</td>\n",
              "      <td>0.031024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>they're some developers who aim to make a fast...</td>\n",
              "      <td>あぶく銭を稼ごうとする開発業者たちよ。</td>\n",
              "      <td>あすスミスさんのお宅に伺うことになっている。</td>\n",
              "      <td>0.027033</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         input_sentence  ... bleu_score\n",
              "0                       four miles is a good distance .  ...   0.046192\n",
              "1                                     you can make it .  ...   0.041130\n",
              "2                               i am happy to see you .  ...   0.046733\n",
              "3     out of the students , only one had read that b...  ...   0.029048\n",
              "4                                that's bill all over .  ...   0.053002\n",
              "...                                                 ...  ...        ...\n",
              "2995  our meeting like that is probably the kind of ...  ...   0.015530\n",
              "2996                  who is the boss of this company ?  ...   0.034934\n",
              "2997                  where on earth did you meet him ?  ...   0.036000\n",
              "2998  the maid was dead tired of her household chores .  ...   0.031024\n",
              "2999  they're some developers who aim to make a fast...  ...   0.027033\n",
              "\n",
              "[3000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9hQ_XKxIQ2W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}