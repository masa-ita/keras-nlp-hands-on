{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QetvaJ27Tb6y",
        "outputId": "c3dd90bb-737d-4a24-fc1d-e6e81e87bb0e"
      },
      "source": [
        "!pip install janome japanize_matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting japanize_matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/85/08a4b7fe8987582d99d9bb7ad0ff1ec75439359a7f9690a0dbf2dbf98b15/japanize-matplotlib-1.1.3.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize_matplotlib) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize_matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->japanize_matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-cp36-none-any.whl size=4120276 sha256=24622c01fb9a73e1c8fe44362fb702ab3414449558135b7afb09b9e9899c32b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/d9/a2/f907d50b32a2d2008ce5d691d30fb6569c2c93eefcfde55202\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: janome, japanize-matplotlib\n",
            "Successfully installed janome-0.4.1 japanize-matplotlib-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M-eDSyL-8E7",
        "outputId": "20ba7826-39a7-437f-c91e-ba6bd8d3bef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "gz_file_path = get_file('examples_pd.gz', 'ftp://ftp.monash.edu/pub/nihongo/examples_pd.gz',)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from ftp://ftp.monash.edu/pub/nihongo/examples_pd.gz\n",
            "8388608/8386099 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsrpAmZ1BVFF"
      },
      "source": [
        "import gzip\n",
        "import pandas as pd\n",
        "\n",
        "with open(gz_file_path, 'rb') as fd:\n",
        "    gzip_fd = gzip.GzipFile(fileobj=fd)\n",
        "    raw_lines = gzip_fd.readlines()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8meT2QYBrTN",
        "outputId": "a7b9fb9f-d896-46ab-cd74-8527f0cb0c1a"
      },
      "source": [
        "import re\n",
        "\n",
        "NUM_EXAMPLES = 30000\n",
        "\n",
        "re_line = re.compile(r'(A|B):\\s(.+)\\t(.+)(?:#ID=\\d+\\r\\n)')\n",
        "\n",
        "en = []\n",
        "ja = []\n",
        "\n",
        "for raw_line in raw_lines[:NUM_EXAMPLES]:\n",
        "    raw_line = raw_line.decode('euc_jisx0213')\n",
        "    m = re_line.match(raw_line)\n",
        "    if m and m[1] == 'A':\n",
        "        ja.append(m[2])\n",
        "        en.append(m[3])\n",
        "print(ja[:5])\n",
        "print(en[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['＆という記号は、ａｎｄを指す。', '＆のマークはａｎｄの文字を表す。', '（自転車に乗って）フーッ、この坂道はきついよ。でも帰りは楽だよね。', '実のところ物価は毎週上昇している。', '〜と痛切に感じている。']\n",
            "[\"The sign '&' stands for 'and'.\", 'The mark \"&\" stands for \"and\".', '(On a bicycle) Whew! This is a tough hill. But coming back sure will be a breeze.', 'As it is, prices are going up every week.', 'I was acutely aware that..']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNHMSVjG4_C"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_en(s):\n",
        "    return unicodedata.normalize('NFD', s)\n",
        "\n",
        "def normalize_ja(s):\n",
        "    return unicodedata.normalize('NFKC', s)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Knl8HkaSUvM"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "t_wakati = Tokenizer(wakati=True)\n",
        "\n",
        "def tokenize_japanese(text):\n",
        "    return ' '.join(list(t_wakati.tokenize(text)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drPlWcVMMj6d"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_en(w):\n",
        "    w = normalize_en(w.lower().strip())\n",
        "\n",
        "    # 単語とそのあとの句読点の間にスペースを挿入\n",
        "    # 例：　\"he is a boy.\" => \"he is a boy .\"\n",
        "    # 参照：- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # 文の開始と終了のトークンを付加\n",
        "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "def preprocess_ja(w):\n",
        "    w = normalize_ja(w)\n",
        "\n",
        "    w = tokenize_japanese(w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # 文の開始と終了のトークンを付加\n",
        "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVVheh_zNpss",
        "outputId": "5b837a62-a33f-4369-b608-359e2939bb94"
      },
      "source": [
        "en_example = \"The sign '&' stands for 'and'.\"\n",
        "ja_example = '＆という記号は、ａｎｄを指す。'\n",
        "print(preprocess_en(en_example))\n",
        "print(preprocess_ja(ja_example))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> the sign '&' stands for 'and' . <end>\n",
            "<start> & という 記号 は 、 and を 指す 。 <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqAiXQJ3TZGs",
        "outputId": "9c2ed406-d3bf-4f85-9e29-b19646a986e4"
      },
      "source": [
        "en = [preprocess_en(s) for s in en]\n",
        "ja = [preprocess_ja(s) for s in ja]\n",
        "print(en[-1])\n",
        "print(ja[-1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> this toothbrush is not used by my mother . <end>\n",
            "<start> この 歯ブラシ を 使っ て いる の は 母 で は ない 。 <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf4fwROcVrVP"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                           padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyvASA-0W3f0"
      },
      "source": [
        "def create_dataset(targ_lang, inp_lang):\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmmoXQBwXO_R"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = create_dataset(ja, en)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGr90EQXZDe"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-HKiv5iXnke"
      },
      "source": [
        "# ターゲットテンソルの最大長を計算\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nT12tjPXtqT",
        "outputId": "c21771c0-4205-4429-cab7-e3eae1c329f3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80-20で分割を行い、訓練用と検証用のデータセットを作成\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# 長さを表示\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12000 12000 3000 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxSreRrXvw4"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBMUz4Q9Xxe7",
        "outputId": "9147badc-14cf-4a26-9985-37188ad5af2f"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "11 ----> this\n",
            "865 ----> data\n",
            "7 ----> is\n",
            "7818 ----> incorrect\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "15 ----> この\n",
            "1032 ----> データ\n",
            "4 ----> は\n",
            "10102 ----> 不正確\n",
            "14 ----> で\n",
            "30 ----> ある\n",
            "3 ----> 。\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYSOGLzYbP8"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkZvi19gYp03",
        "outputId": "0ef2a603-9a43-46c2-e991-42905f2c91c7"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 47]), TensorShape([64, 66]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYHYgc4hYwly"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units,):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, *states = self.lstm(x)\n",
        "        return output, states"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5fs_ECmZP8N",
        "outputId": "03715c08-203b-4fb6-e184-ae8610c57b46"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units,)\n",
        "\n",
        "# サンプル入力\n",
        "sample_output, sample_hidden = encoder(example_input_batch)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
        "print ('Encoder Carry state shape: (batch size, units) {}'.format(sample_hidden[1].shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 47, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Encoder Carry state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRy4fGRXZU9P"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # スコアを計算するためにこのように加算を実行する\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # スコアを self.V に適用するために最後の軸は 1 となる\n",
        "        # self.V に適用する前のテンソルの shape は  (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                      self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights の shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector の合計後の shape == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaX0KziyZoN5",
        "outputId": "11c9faf0-614e-4ca7-ae4e-e73e71cf4fa8"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden[0], sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 47, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuNjW0lZs1y"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # アテンションのため\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output の shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n",
        "\n",
        "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 結合後の x の shape == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # 結合したベクトルを LSTM 層に渡す\n",
        "        output, *states = self.lstm(x, initial_state=hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, states, attention_weights"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP8fSgqDZ9Ci",
        "outputId": "45c84b20-ac7a-4a64-8ca3-bb8e60bcd9f6"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 10804)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOmW4oGQaBuJ"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                             from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1xOnT6oaOCY"
      },
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaknH7tgaTEt"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher Forcing - 正解値を次の入力として供給\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Teacher Forcing を使用\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bNwa8eCa0Ru",
        "outputId": "5cfb7674-eead-4d13-85c2-8275f4e59c8f"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.7062\n",
            "Epoch 1 Batch 100 Loss 1.2842\n",
            "Epoch 1 Loss 1.1249\n",
            "Time taken for 1 epoch 213.76039695739746 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1227\n",
            "Epoch 2 Batch 100 Loss 0.8433\n",
            "Epoch 2 Loss 0.8916\n",
            "Time taken for 1 epoch 146.8460717201233 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7251\n",
            "Epoch 3 Batch 100 Loss 0.7074\n",
            "Epoch 3 Loss 0.7606\n",
            "Time taken for 1 epoch 145.63818097114563 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7181\n",
            "Epoch 4 Batch 100 Loss 0.7142\n",
            "Epoch 4 Loss 0.6995\n",
            "Time taken for 1 epoch 146.67460107803345 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6403\n",
            "Epoch 5 Batch 100 Loss 0.6957\n",
            "Epoch 5 Loss 0.6533\n",
            "Time taken for 1 epoch 145.57571172714233 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5889\n",
            "Epoch 6 Batch 100 Loss 0.6409\n",
            "Epoch 6 Loss 0.6140\n",
            "Time taken for 1 epoch 146.6535346508026 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6357\n",
            "Epoch 7 Batch 100 Loss 0.6334\n",
            "Epoch 7 Loss 0.5782\n",
            "Time taken for 1 epoch 145.4954171180725 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5387\n",
            "Epoch 8 Batch 100 Loss 0.5907\n",
            "Epoch 8 Loss 0.5437\n",
            "Time taken for 1 epoch 146.59597754478455 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5057\n",
            "Epoch 9 Batch 100 Loss 0.5173\n",
            "Epoch 9 Loss 0.5115\n",
            "Time taken for 1 epoch 145.61827445030212 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4608\n",
            "Epoch 10 Batch 100 Loss 0.4488\n",
            "Epoch 10 Loss 0.4796\n",
            "Time taken for 1 epoch 146.61339211463928 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.4397\n",
            "Epoch 11 Batch 100 Loss 0.4826\n",
            "Epoch 11 Loss 0.4484\n",
            "Time taken for 1 epoch 145.81100058555603 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.3933\n",
            "Epoch 12 Batch 100 Loss 0.4268\n",
            "Epoch 12 Loss 0.4213\n",
            "Time taken for 1 epoch 147.26446342468262 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3810\n",
            "Epoch 13 Batch 100 Loss 0.3748\n",
            "Epoch 13 Loss 0.3919\n",
            "Time taken for 1 epoch 146.29451704025269 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.3433\n",
            "Epoch 14 Batch 100 Loss 0.3720\n",
            "Epoch 14 Loss 0.3624\n",
            "Time taken for 1 epoch 147.0678403377533 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2880\n",
            "Epoch 15 Batch 100 Loss 0.3542\n",
            "Epoch 15 Loss 0.3335\n",
            "Time taken for 1 epoch 146.18724465370178 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.2961\n",
            "Epoch 16 Batch 100 Loss 0.3334\n",
            "Epoch 16 Loss 0.3068\n",
            "Time taken for 1 epoch 147.39225506782532 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2781\n",
            "Epoch 17 Batch 100 Loss 0.2942\n",
            "Epoch 17 Loss 0.2811\n",
            "Time taken for 1 epoch 146.64033436775208 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2697\n",
            "Epoch 18 Batch 100 Loss 0.2531\n",
            "Epoch 18 Loss 0.2573\n",
            "Time taken for 1 epoch 147.6647036075592 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2269\n",
            "Epoch 19 Batch 100 Loss 0.2441\n",
            "Epoch 19 Loss 0.2350\n",
            "Time taken for 1 epoch 146.5680730342865 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1952\n",
            "Epoch 20 Batch 100 Loss 0.2714\n",
            "Epoch 20 Loss 0.2372\n",
            "Time taken for 1 epoch 147.9230535030365 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6kO2HY0bEn0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_en(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # 後ほどプロットするためにアテンションの重みを保存\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnwoWfkYfwBZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import japanize_matplotlib\n",
        "\n",
        "# アテンションの重みをプロットする関数\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zIxy2Zf3vw"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIbzsxI-f8Z9",
        "outputId": "20269f02-d2fe-458a-e5c9-f30aa91ba8e4"
      },
      "source": [
        "# checkpoint_dir の中の最後のチェックポイントを復元\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5e1fff19e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "aX4evgWVf9xa",
        "outputId": "c16275ab-be6a-4c7d-ee3f-5e385adeb33d"
      },
      "source": [
        "translate('It is necessary that the bill pass the diet.')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> it is necessary that the bill pass the diet . <end>\n",
            "Predicted translation: あなた は その こと を どう 処理 し た の か 。 <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7_xsAt-QWXl"
      },
      "source": [
        "def predict(inputs):\n",
        "    inputs = tf.convert_to_tensor([inputs])\n",
        "    predicted_seq = []\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # 後ほどプロットするためにアテンションの重みを保存\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        predicted_seq.append(predicted_id)\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return predicted_seq\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return predicted_seq\n",
        "    "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qYv1rYq1Lu5"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "input_sentences = []\n",
        "target_sentences = []\n",
        "predicted_sentences = []\n",
        "\n",
        "for i, input_en in enumerate(input_tensor_val):\n",
        "    predicted_ja = predict(input_en)\n",
        "    tokens_input = [inp_lang.index_word[id] for id in input_en if id > 2]\n",
        "    tokens_target = [targ_lang.index_word[id] for id in target_tensor_val[i] if id > 2]\n",
        "    tokens_predicted = [targ_lang.index_word[id] for id in predicted_ja if id > 2]\n",
        "    input_sentences.append(' '.join(tokens_input))\n",
        "    target_sentences.append(''.join(tokens_target))\n",
        "    predicted_sentences.append(''.join(tokens_predicted))\n",
        "\n",
        "result_df = pd.DataFrame({'input_sentence': input_sentences,\n",
        "                          'target_sentence': target_sentences,\n",
        "                          'predicted_sentence': predicted_sentences})  \n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COnhI7VyQWXp"
      },
      "source": [
        "bleu_scores = []\n",
        "for row in result_df.itertuples():\n",
        "    bleu_scores.append(\n",
        "        sentence_bleu(row.target_sentence, row.predicted_sentence,\n",
        "                      smoothing_function=SmoothingFunction().method3)\n",
        "    )\n",
        "result_df['bleu_score'] = bleu_scores"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOhwVunpl0nR",
        "outputId": "5fbdffa7-8f95-4d22-bb3e-bc6eca754d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "result_df"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "      <th>predicted_sentence</th>\n",
              "      <th>bleu_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>his wife speaks spanish as well as english .</td>\n",
              "      <td>あの人の奥さんは、英語だけでなくスペイン語も話します。</td>\n",
              "      <td>あの人の人たちはおもしろくない。</td>\n",
              "      <td>0.042359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you are nodding over your work .</td>\n",
              "      <td>あなたはお仕事しながらこっくりしてますよ。</td>\n",
              "      <td>あなたはそのことをどう処理したのか。</td>\n",
              "      <td>0.034934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>take the book which is lying on that table .</td>\n",
              "      <td>あのテーブルの上にある本を取って下さい。</td>\n",
              "      <td>あなたのお宅の皆様によろしく。</td>\n",
              "      <td>0.037165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>let's drink the cup of tea here .</td>\n",
              "      <td>ここで紅茶を飲みましょう。</td>\n",
              "      <td>ここでは喫煙にご遠慮下さい。</td>\n",
              "      <td>0.037374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>there are a number of places to see in this ci...</td>\n",
              "      <td>この市には見物する所がかなりある。</td>\n",
              "      <td>このようなことが二度と起こらないよう努力します。</td>\n",
              "      <td>0.024623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>i may have caught cold on that cold night .</td>\n",
              "      <td>あの寒い夜に風邪を引いたのかもしれない。</td>\n",
              "      <td>この夏は雷が多かった。</td>\n",
              "      <td>0.053002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>it seems like yesterday that we skated together .</td>\n",
              "      <td>いっしょにスケートをしたのが、まるで昨日のことみたいです。</td>\n",
              "      <td>あの人たちにはやさしすぎました。</td>\n",
              "      <td>0.039779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>you have a standing invitation to join us .</td>\n",
              "      <td>いつおいでくださっても歓迎いたします。</td>\n",
              "      <td>あなたはそのことを後悔するだろう。</td>\n",
              "      <td>0.032342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>koko herself cannot use spoken language , but ...</td>\n",
              "      <td>ココ自身は話し言葉を使えないが、彼女は人々の会話を聞くのが大好きである。</td>\n",
              "      <td>あの人たちの小屋は谷間にある。</td>\n",
              "      <td>0.041130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>you can use this computer any time .</td>\n",
              "      <td>いつでもこのコンピューターを使っていいよ。</td>\n",
              "      <td>あなたはそのことをどう処理したのか。</td>\n",
              "      <td>0.030373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         input_sentence  ... bleu_score\n",
              "0          his wife speaks spanish as well as english .  ...   0.042359\n",
              "1                      you are nodding over your work .  ...   0.034934\n",
              "2          take the book which is lying on that table .  ...   0.037165\n",
              "3                     let's drink the cup of tea here .  ...   0.037374\n",
              "4     there are a number of places to see in this ci...  ...   0.024623\n",
              "...                                                 ...  ...        ...\n",
              "2995        i may have caught cold on that cold night .  ...   0.053002\n",
              "2996  it seems like yesterday that we skated together .  ...   0.039779\n",
              "2997        you have a standing invitation to join us .  ...   0.032342\n",
              "2998  koko herself cannot use spoken language , but ...  ...   0.041130\n",
              "2999               you can use this computer any time .  ...   0.030373\n",
              "\n",
              "[3000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-NJdWuVpHl3",
        "outputId": "44a23bbd-aea6-4e89-f310-e5abeb6ca350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dir(result_df)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T',\n",
              " '_AXIS_LEN',\n",
              " '_AXIS_NAMES',\n",
              " '_AXIS_NUMBERS',\n",
              " '_AXIS_ORDERS',\n",
              " '_AXIS_REVERSED',\n",
              " '_AXIS_TO_AXIS_NUMBER',\n",
              " '__abs__',\n",
              " '__add__',\n",
              " '__and__',\n",
              " '__annotations__',\n",
              " '__array__',\n",
              " '__array_priority__',\n",
              " '__array_wrap__',\n",
              " '__bool__',\n",
              " '__class__',\n",
              " '__contains__',\n",
              " '__copy__',\n",
              " '__deepcopy__',\n",
              " '__delattr__',\n",
              " '__delitem__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__div__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__finalize__',\n",
              " '__floordiv__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__iand__',\n",
              " '__ifloordiv__',\n",
              " '__imod__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__invert__',\n",
              " '__ior__',\n",
              " '__ipow__',\n",
              " '__isub__',\n",
              " '__iter__',\n",
              " '__itruediv__',\n",
              " '__ixor__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__matmul__',\n",
              " '__mod__',\n",
              " '__module__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__neg__',\n",
              " '__new__',\n",
              " '__nonzero__',\n",
              " '__or__',\n",
              " '__pos__',\n",
              " '__pow__',\n",
              " '__radd__',\n",
              " '__rand__',\n",
              " '__rdiv__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__rfloordiv__',\n",
              " '__rmatmul__',\n",
              " '__rmod__',\n",
              " '__rmul__',\n",
              " '__ror__',\n",
              " '__round__',\n",
              " '__rpow__',\n",
              " '__rsub__',\n",
              " '__rtruediv__',\n",
              " '__rxor__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__truediv__',\n",
              " '__weakref__',\n",
              " '__xor__',\n",
              " '_accessors',\n",
              " '_add_numeric_operations',\n",
              " '_add_series_or_dataframe_operations',\n",
              " '_agg_by_level',\n",
              " '_agg_examples_doc',\n",
              " '_agg_summary_and_see_also_doc',\n",
              " '_aggregate',\n",
              " '_aggregate_multiple_funcs',\n",
              " '_align_frame',\n",
              " '_align_series',\n",
              " '_box_col_values',\n",
              " '_builtin_table',\n",
              " '_can_fast_transpose',\n",
              " '_check_inplace_setting',\n",
              " '_check_is_chained_assignment_possible',\n",
              " '_check_label_or_level_ambiguity',\n",
              " '_check_setitem_copy',\n",
              " '_clear_item_cache',\n",
              " '_clip_with_one_bound',\n",
              " '_clip_with_scalar',\n",
              " '_combine_frame',\n",
              " '_consolidate',\n",
              " '_consolidate_inplace',\n",
              " '_construct_axes_dict',\n",
              " '_construct_axes_from_arguments',\n",
              " '_construct_result',\n",
              " '_constructor',\n",
              " '_constructor_expanddim',\n",
              " '_constructor_sliced',\n",
              " '_convert',\n",
              " '_count_level',\n",
              " '_cython_table',\n",
              " '_data',\n",
              " '_deprecations',\n",
              " '_dir_additions',\n",
              " '_dir_deletions',\n",
              " '_drop_axis',\n",
              " '_drop_labels_or_levels',\n",
              " '_ensure_valid_index',\n",
              " '_find_valid_index',\n",
              " '_from_arrays',\n",
              " '_get_agg_axis',\n",
              " '_get_axis',\n",
              " '_get_axis_name',\n",
              " '_get_axis_number',\n",
              " '_get_axis_resolvers',\n",
              " '_get_block_manager_axis',\n",
              " '_get_bool_data',\n",
              " '_get_cacher',\n",
              " '_get_cleaned_column_resolvers',\n",
              " '_get_column_array',\n",
              " '_get_cython_func',\n",
              " '_get_index_resolvers',\n",
              " '_get_item_cache',\n",
              " '_get_label_or_level_values',\n",
              " '_get_numeric_data',\n",
              " '_get_value',\n",
              " '_getitem_bool_array',\n",
              " '_getitem_multilevel',\n",
              " '_gotitem',\n",
              " '_indexed_same',\n",
              " '_info_axis',\n",
              " '_info_axis_name',\n",
              " '_info_axis_number',\n",
              " '_info_repr',\n",
              " '_init_mgr',\n",
              " '_internal_names',\n",
              " '_internal_names_set',\n",
              " '_is_builtin_func',\n",
              " '_is_cached',\n",
              " '_is_copy',\n",
              " '_is_homogeneous_type',\n",
              " '_is_label_or_level_reference',\n",
              " '_is_label_reference',\n",
              " '_is_level_reference',\n",
              " '_is_mixed_type',\n",
              " '_is_view',\n",
              " '_iset_item',\n",
              " '_iter_column_arrays',\n",
              " '_ix',\n",
              " '_ixs',\n",
              " '_join_compat',\n",
              " '_maybe_cache_changed',\n",
              " '_maybe_update_cacher',\n",
              " '_metadata',\n",
              " '_needs_reindex_multi',\n",
              " '_obj_with_exclusions',\n",
              " '_protect_consolidate',\n",
              " '_reduce',\n",
              " '_reindex_axes',\n",
              " '_reindex_columns',\n",
              " '_reindex_index',\n",
              " '_reindex_multi',\n",
              " '_reindex_with_indexers',\n",
              " '_replace_columnwise',\n",
              " '_repr_data_resource_',\n",
              " '_repr_fits_horizontal_',\n",
              " '_repr_fits_vertical_',\n",
              " '_repr_html_',\n",
              " '_repr_latex_',\n",
              " '_reset_cache',\n",
              " '_reset_cacher',\n",
              " '_sanitize_column',\n",
              " '_selected_obj',\n",
              " '_selection',\n",
              " '_selection_list',\n",
              " '_selection_name',\n",
              " '_series',\n",
              " '_set_as_cached',\n",
              " '_set_axis',\n",
              " '_set_axis_name',\n",
              " '_set_is_copy',\n",
              " '_set_item',\n",
              " '_set_value',\n",
              " '_setitem_array',\n",
              " '_setitem_frame',\n",
              " '_setitem_slice',\n",
              " '_slice',\n",
              " '_stat_axis',\n",
              " '_stat_axis_name',\n",
              " '_stat_axis_number',\n",
              " '_take_with_is_copy',\n",
              " '_to_dict_of_blocks',\n",
              " '_try_aggregate_string_function',\n",
              " '_typ',\n",
              " '_update_inplace',\n",
              " '_validate_dtype',\n",
              " '_values',\n",
              " '_where',\n",
              " 'abs',\n",
              " 'add',\n",
              " 'add_prefix',\n",
              " 'add_suffix',\n",
              " 'agg',\n",
              " 'aggregate',\n",
              " 'align',\n",
              " 'all',\n",
              " 'any',\n",
              " 'append',\n",
              " 'apply',\n",
              " 'applymap',\n",
              " 'asfreq',\n",
              " 'asof',\n",
              " 'assign',\n",
              " 'astype',\n",
              " 'at',\n",
              " 'at_time',\n",
              " 'attrs',\n",
              " 'axes',\n",
              " 'backfill',\n",
              " 'between_time',\n",
              " 'bfill',\n",
              " 'bleu_score',\n",
              " 'bool',\n",
              " 'boxplot',\n",
              " 'clip',\n",
              " 'columns',\n",
              " 'combine',\n",
              " 'combine_first',\n",
              " 'compare',\n",
              " 'convert_dtypes',\n",
              " 'copy',\n",
              " 'corr',\n",
              " 'corrwith',\n",
              " 'count',\n",
              " 'cov',\n",
              " 'cummax',\n",
              " 'cummin',\n",
              " 'cumprod',\n",
              " 'cumsum',\n",
              " 'describe',\n",
              " 'diff',\n",
              " 'div',\n",
              " 'divide',\n",
              " 'dot',\n",
              " 'drop',\n",
              " 'drop_duplicates',\n",
              " 'droplevel',\n",
              " 'dropna',\n",
              " 'dtypes',\n",
              " 'duplicated',\n",
              " 'empty',\n",
              " 'eq',\n",
              " 'equals',\n",
              " 'eval',\n",
              " 'ewm',\n",
              " 'expanding',\n",
              " 'explode',\n",
              " 'ffill',\n",
              " 'fillna',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'first_valid_index',\n",
              " 'floordiv',\n",
              " 'from_dict',\n",
              " 'from_records',\n",
              " 'ge',\n",
              " 'get',\n",
              " 'groupby',\n",
              " 'gt',\n",
              " 'head',\n",
              " 'hist',\n",
              " 'iat',\n",
              " 'idxmax',\n",
              " 'idxmin',\n",
              " 'iloc',\n",
              " 'index',\n",
              " 'infer_objects',\n",
              " 'info',\n",
              " 'input_sentence',\n",
              " 'insert',\n",
              " 'interpolate',\n",
              " 'isin',\n",
              " 'isna',\n",
              " 'isnull',\n",
              " 'items',\n",
              " 'iteritems',\n",
              " 'iterrows',\n",
              " 'itertuples',\n",
              " 'join',\n",
              " 'keys',\n",
              " 'kurt',\n",
              " 'kurtosis',\n",
              " 'last',\n",
              " 'last_valid_index',\n",
              " 'le',\n",
              " 'loc',\n",
              " 'lookup',\n",
              " 'lt',\n",
              " 'mad',\n",
              " 'mask',\n",
              " 'max',\n",
              " 'mean',\n",
              " 'median',\n",
              " 'melt',\n",
              " 'memory_usage',\n",
              " 'merge',\n",
              " 'min',\n",
              " 'mod',\n",
              " 'mode',\n",
              " 'mul',\n",
              " 'multiply',\n",
              " 'ndim',\n",
              " 'ne',\n",
              " 'nlargest',\n",
              " 'notna',\n",
              " 'notnull',\n",
              " 'nsmallest',\n",
              " 'nunique',\n",
              " 'pad',\n",
              " 'pct_change',\n",
              " 'pipe',\n",
              " 'pivot',\n",
              " 'pivot_table',\n",
              " 'plot',\n",
              " 'pop',\n",
              " 'pow',\n",
              " 'predicted_sentence',\n",
              " 'prod',\n",
              " 'product',\n",
              " 'quantile',\n",
              " 'query',\n",
              " 'radd',\n",
              " 'rank',\n",
              " 'rdiv',\n",
              " 'reindex',\n",
              " 'reindex_like',\n",
              " 'rename',\n",
              " 'rename_axis',\n",
              " 'reorder_levels',\n",
              " 'replace',\n",
              " 'resample',\n",
              " 'reset_index',\n",
              " 'rfloordiv',\n",
              " 'rmod',\n",
              " 'rmul',\n",
              " 'rolling',\n",
              " 'round',\n",
              " 'rpow',\n",
              " 'rsub',\n",
              " 'rtruediv',\n",
              " 'sample',\n",
              " 'select_dtypes',\n",
              " 'sem',\n",
              " 'set_axis',\n",
              " 'set_index',\n",
              " 'shape',\n",
              " 'shift',\n",
              " 'size',\n",
              " 'skew',\n",
              " 'slice_shift',\n",
              " 'sort_index',\n",
              " 'sort_values',\n",
              " 'squeeze',\n",
              " 'stack',\n",
              " 'std',\n",
              " 'style',\n",
              " 'sub',\n",
              " 'subtract',\n",
              " 'sum',\n",
              " 'swapaxes',\n",
              " 'swaplevel',\n",
              " 'tail',\n",
              " 'take',\n",
              " 'target_sentence',\n",
              " 'to_clipboard',\n",
              " 'to_csv',\n",
              " 'to_dict',\n",
              " 'to_excel',\n",
              " 'to_feather',\n",
              " 'to_gbq',\n",
              " 'to_hdf',\n",
              " 'to_html',\n",
              " 'to_json',\n",
              " 'to_latex',\n",
              " 'to_markdown',\n",
              " 'to_numpy',\n",
              " 'to_parquet',\n",
              " 'to_period',\n",
              " 'to_pickle',\n",
              " 'to_records',\n",
              " 'to_sql',\n",
              " 'to_stata',\n",
              " 'to_string',\n",
              " 'to_timestamp',\n",
              " 'to_xarray',\n",
              " 'transform',\n",
              " 'transpose',\n",
              " 'truediv',\n",
              " 'truncate',\n",
              " 'tz_convert',\n",
              " 'tz_localize',\n",
              " 'unstack',\n",
              " 'update',\n",
              " 'value_counts',\n",
              " 'values',\n",
              " 'var',\n",
              " 'where',\n",
              " 'xs']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCRzYCWVqyk6",
        "outputId": "c10aaa8f-8ff0-4711-f1ab-50a5b7921cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result_df.bleu_score.mean()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.035047338544287376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}