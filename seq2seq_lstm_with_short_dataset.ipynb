{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M-eDSyL-8E7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f42f59e-e667-4f23-fa16-57d82dd3e4aa"
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "url = 'https://github.com/odashi/small_parallel_enja/archive/master.zip'\n",
        "\n",
        "zip_file_path = get_file('small_parallel_enja.zip', url, cache_subdir='small_parallel_enja', extract=True) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/odashi/small_parallel_enja/archive/master.zip\n",
            "2547712/Unknown - 2s 1us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsrpAmZ1BVFF",
        "outputId": "4f86b029-427d-4f1a-e5bc-89a286ec9b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(zip_file_path), 'small_parallel_enja-master')\n",
        "!ls -l $data_dir"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 9076\n",
            "-rw-r--r-- 1 root root   17054 Feb  8 05:49 dev.en\n",
            "-rw-r--r-- 1 root root   27781 Feb  8 05:49 dev.ja\n",
            "-rw-r--r-- 1 root root    1946 Feb  8 05:49 README.md\n",
            "-rw-r--r-- 1 root root   17301 Feb  8 05:49 test.en\n",
            "-rw-r--r-- 1 root root   27793 Feb  8 05:49 test.ja\n",
            "-rw-r--r-- 1 root root 1701356 Feb  8 05:49 train.en\n",
            "-rw-r--r-- 1 root root  339768 Feb  8 05:49 train.en.000\n",
            "-rw-r--r-- 1 root root  340186 Feb  8 05:49 train.en.001\n",
            "-rw-r--r-- 1 root root  341174 Feb  8 05:49 train.en.002\n",
            "-rw-r--r-- 1 root root  339953 Feb  8 05:49 train.en.003\n",
            "-rw-r--r-- 1 root root  340275 Feb  8 05:49 train.en.004\n",
            "-rw-r--r-- 1 root root   30025 Feb  8 05:49 train.en.vocab.4k\n",
            "-rw-r--r-- 1 root root   51162 Feb  8 05:49 train.en.vocab.all\n",
            "-rw-r--r-- 1 root root 2784447 Feb  8 05:49 train.ja\n",
            "-rw-r--r-- 1 root root  556444 Feb  8 05:49 train.ja.000\n",
            "-rw-r--r-- 1 root root  555732 Feb  8 05:49 train.ja.001\n",
            "-rw-r--r-- 1 root root  557218 Feb  8 05:49 train.ja.002\n",
            "-rw-r--r-- 1 root root  557538 Feb  8 05:49 train.ja.003\n",
            "-rw-r--r-- 1 root root  557515 Feb  8 05:49 train.ja.004\n",
            "-rw-r--r-- 1 root root   31009 Feb  8 05:49 train.ja.vocab.4k\n",
            "-rw-r--r-- 1 root root   73669 Feb  8 05:49 train.ja.vocab.all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3iF78mDMb1B"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def load_data(path):\n",
        "      tokenizer = Tokenizer(filters='')\n",
        "      texts = []\n",
        "      for line in open(path, 'r'):\n",
        "          texts.append('<start> ' + line.strip() + ' <end>')\n",
        "    \n",
        "      tokenizer.fit_on_texts(texts)\n",
        "\n",
        "      return tokenizer.texts_to_sequences(texts), tokenizer\n",
        "\n",
        "en, inp_lang = load_data(os.path.join(data_dir, 'train.en'))\n",
        "ja, targ_lang = load_data(os.path.join(data_dir, 'train.ja'))\n",
        "\n",
        "vocab_inp_size = len(tokenizer_en.word_index) + 1\n",
        "vocab_tar_size = len(tokenizer_ja.word_index) + 1\n",
        "\n",
        "train_en, test_en, train_ja, test_ja = train_test_split(en, ja, test_size=0.1, random_state=36)\n",
        "\n",
        "input_tensor_train = pad_sequences(train_en, padding='post')\n",
        "target_tensor_train = pad_sequences(train_ja, padding='post')\n",
        "input_tensor_val = pad_sequences(test_en, padding='post')\n",
        "target_tensor_val = pad_sequences(test_ja, padding='post')\n",
        "\n",
        "max_length_inp = len(input_tensor_train[0])\n",
        "max_length_targ = len(target_tensor_train[0])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfxSreRrXvw4"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBMUz4Q9Xxe7",
        "outputId": "785c0b4d-3ab3-47e3-9952-62ebf205dcf3"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "13 ----> it\n",
            "33 ----> 's\n",
            "918 ----> dangerous\n",
            "6 ----> to\n",
            "182 ----> play\n",
            "387 ----> around\n",
            "4 ----> the\n",
            "459 ----> fire\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "718 ----> 火\n",
            "9 ----> の\n",
            "459 ----> 近く\n",
            "11 ----> で\n",
            "382 ----> 遊\n",
            "344 ----> ぶ\n",
            "9 ----> の\n",
            "4 ----> は\n",
            "457 ----> 危険\n",
            "22 ----> だ\n",
            "3 ----> 。\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYSOGLzYbP8"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkZvi19gYp03",
        "outputId": "50455766-a592-4c4c-e917-234904cfc969"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 18]), TensorShape([64, 18]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYHYgc4hYwly"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=False,\n",
        "                                       return_state=True)\n",
        " \n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, *states = self.lstm(x)\n",
        "        return output, states\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5fs_ECmZP8N",
        "outputId": "6bc3d40f-5eae-4e55-85f5-627b0290ce25"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
        "\n",
        "# サンプル入力\n",
        "sample_output, sample_hidden = encoder(example_input_batch)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
        "print ('Encoder Carry state shape: (batch size, units) {}'.format(sample_hidden[1].shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Encoder Carry state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzuNjW0lZs1y"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Embeddingの出力と、エンコーダ出力を LSTM 層に渡す\n",
        "        output, *states = self.lstm(x, initial_state=hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, states"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP8fSgqDZ9Ci",
        "outputId": "8e5ffe69-882b-4f15-d3bd-578bae27f6a5"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
        "\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 8777)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOmW4oGQaBuJ"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                             from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1xOnT6oaOCY"
      },
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaknH7tgaTEt"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher Forcing - 正解値を次の入力として供給\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Teacher Forcing を使用\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bNwa8eCa0Ru",
        "outputId": "ffd54be8-8926-4d40-b880-b8fb7a14d8e7"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 6.0849\n",
            "Epoch 1 Batch 100 Loss 2.7703\n",
            "Epoch 1 Batch 200 Loss 2.3135\n",
            "Epoch 1 Batch 300 Loss 2.1015\n",
            "Epoch 1 Batch 400 Loss 2.1106\n",
            "Epoch 1 Batch 500 Loss 2.0142\n",
            "Epoch 1 Batch 600 Loss 2.2089\n",
            "Epoch 1 Loss 2.3625\n",
            "Time taken for 1 epoch 80.29868793487549 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8620\n",
            "Epoch 2 Batch 100 Loss 1.9529\n",
            "Epoch 2 Batch 200 Loss 1.8381\n",
            "Epoch 2 Batch 300 Loss 1.7275\n",
            "Epoch 2 Batch 400 Loss 1.7937\n",
            "Epoch 2 Batch 500 Loss 1.6890\n",
            "Epoch 2 Batch 600 Loss 1.7004\n",
            "Epoch 2 Loss 1.7626\n",
            "Time taken for 1 epoch 63.54192876815796 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4768\n",
            "Epoch 3 Batch 100 Loss 1.5853\n",
            "Epoch 3 Batch 200 Loss 1.5798\n",
            "Epoch 3 Batch 300 Loss 1.5546\n",
            "Epoch 3 Batch 400 Loss 1.5519\n",
            "Epoch 3 Batch 500 Loss 1.4247\n",
            "Epoch 3 Batch 600 Loss 1.4377\n",
            "Epoch 3 Loss 1.4843\n",
            "Time taken for 1 epoch 62.92848563194275 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3223\n",
            "Epoch 4 Batch 100 Loss 1.2289\n",
            "Epoch 4 Batch 200 Loss 1.3007\n",
            "Epoch 4 Batch 300 Loss 1.3095\n",
            "Epoch 4 Batch 400 Loss 1.1648\n",
            "Epoch 4 Batch 500 Loss 1.1917\n",
            "Epoch 4 Batch 600 Loss 1.3859\n",
            "Epoch 4 Loss 1.2466\n",
            "Time taken for 1 epoch 63.34821581840515 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.0176\n",
            "Epoch 5 Batch 100 Loss 0.9885\n",
            "Epoch 5 Batch 200 Loss 1.1614\n",
            "Epoch 5 Batch 300 Loss 1.0844\n",
            "Epoch 5 Batch 400 Loss 0.9739\n",
            "Epoch 5 Batch 500 Loss 0.9307\n",
            "Epoch 5 Batch 600 Loss 1.0637\n",
            "Epoch 5 Loss 1.0373\n",
            "Time taken for 1 epoch 62.851338148117065 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9020\n",
            "Epoch 6 Batch 100 Loss 0.8688\n",
            "Epoch 6 Batch 200 Loss 0.8085\n",
            "Epoch 6 Batch 300 Loss 0.8126\n",
            "Epoch 6 Batch 400 Loss 0.8842\n",
            "Epoch 6 Batch 500 Loss 0.9082\n",
            "Epoch 6 Batch 600 Loss 0.9149\n",
            "Epoch 6 Loss 0.8608\n",
            "Time taken for 1 epoch 63.05033588409424 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6774\n",
            "Epoch 7 Batch 100 Loss 0.6979\n",
            "Epoch 7 Batch 200 Loss 0.7350\n",
            "Epoch 7 Batch 300 Loss 0.8014\n",
            "Epoch 7 Batch 400 Loss 0.7457\n",
            "Epoch 7 Batch 500 Loss 0.6737\n",
            "Epoch 7 Batch 600 Loss 0.6888\n",
            "Epoch 7 Loss 0.7105\n",
            "Time taken for 1 epoch 62.46406030654907 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5665\n",
            "Epoch 8 Batch 100 Loss 0.5832\n",
            "Epoch 8 Batch 200 Loss 0.5827\n",
            "Epoch 8 Batch 300 Loss 0.5203\n",
            "Epoch 8 Batch 400 Loss 0.5827\n",
            "Epoch 8 Batch 500 Loss 0.5744\n",
            "Epoch 8 Batch 600 Loss 0.5808\n",
            "Epoch 8 Loss 0.5796\n",
            "Time taken for 1 epoch 62.94227075576782 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4075\n",
            "Epoch 9 Batch 100 Loss 0.4031\n",
            "Epoch 9 Batch 200 Loss 0.4678\n",
            "Epoch 9 Batch 300 Loss 0.4202\n",
            "Epoch 9 Batch 400 Loss 0.4530\n",
            "Epoch 9 Batch 500 Loss 0.5219\n",
            "Epoch 9 Batch 600 Loss 0.5129\n",
            "Epoch 9 Loss 0.4649\n",
            "Time taken for 1 epoch 62.419843435287476 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3953\n",
            "Epoch 10 Batch 100 Loss 0.3529\n",
            "Epoch 10 Batch 200 Loss 0.3340\n",
            "Epoch 10 Batch 300 Loss 0.3755\n",
            "Epoch 10 Batch 400 Loss 0.3303\n",
            "Epoch 10 Batch 500 Loss 0.4274\n",
            "Epoch 10 Batch 600 Loss 0.3928\n",
            "Epoch 10 Loss 0.3640\n",
            "Time taken for 1 epoch 63.07679772377014 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2488\n",
            "Epoch 11 Batch 100 Loss 0.2638\n",
            "Epoch 11 Batch 200 Loss 0.2884\n",
            "Epoch 11 Batch 300 Loss 0.2828\n",
            "Epoch 11 Batch 400 Loss 0.2712\n",
            "Epoch 11 Batch 500 Loss 0.3202\n",
            "Epoch 11 Batch 600 Loss 0.3104\n",
            "Epoch 11 Loss 0.2778\n",
            "Time taken for 1 epoch 62.49495267868042 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1900\n",
            "Epoch 12 Batch 100 Loss 0.1964\n",
            "Epoch 12 Batch 200 Loss 0.2162\n",
            "Epoch 12 Batch 300 Loss 0.2207\n",
            "Epoch 12 Batch 400 Loss 0.2346\n",
            "Epoch 12 Batch 500 Loss 0.2557\n",
            "Epoch 12 Batch 600 Loss 0.2650\n",
            "Epoch 12 Loss 0.2075\n",
            "Time taken for 1 epoch 62.89519810676575 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1597\n",
            "Epoch 13 Batch 100 Loss 0.1474\n",
            "Epoch 13 Batch 200 Loss 0.1362\n",
            "Epoch 13 Batch 300 Loss 0.1466\n",
            "Epoch 13 Batch 400 Loss 0.1615\n",
            "Epoch 13 Batch 500 Loss 0.1750\n",
            "Epoch 13 Batch 600 Loss 0.1731\n",
            "Epoch 13 Loss 0.1528\n",
            "Time taken for 1 epoch 62.677916288375854 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1100\n",
            "Epoch 14 Batch 100 Loss 0.1041\n",
            "Epoch 14 Batch 200 Loss 0.1039\n",
            "Epoch 14 Batch 300 Loss 0.1116\n",
            "Epoch 14 Batch 400 Loss 0.1213\n",
            "Epoch 14 Batch 500 Loss 0.1104\n",
            "Epoch 14 Batch 600 Loss 0.1332\n",
            "Epoch 14 Loss 0.1136\n",
            "Time taken for 1 epoch 63.29576587677002 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0876\n",
            "Epoch 15 Batch 100 Loss 0.0799\n",
            "Epoch 15 Batch 200 Loss 0.0751\n",
            "Epoch 15 Batch 300 Loss 0.0793\n",
            "Epoch 15 Batch 400 Loss 0.0853\n",
            "Epoch 15 Batch 500 Loss 0.1012\n",
            "Epoch 15 Batch 600 Loss 0.0994\n",
            "Epoch 15 Loss 0.0866\n",
            "Time taken for 1 epoch 62.4077045917511 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0614\n",
            "Epoch 16 Batch 100 Loss 0.0535\n",
            "Epoch 16 Batch 200 Loss 0.0684\n",
            "Epoch 16 Batch 300 Loss 0.0907\n",
            "Epoch 16 Batch 400 Loss 0.0671\n",
            "Epoch 16 Batch 500 Loss 0.0935\n",
            "Epoch 16 Batch 600 Loss 0.0973\n",
            "Epoch 16 Loss 0.0702\n",
            "Time taken for 1 epoch 63.019622564315796 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0438\n",
            "Epoch 17 Batch 100 Loss 0.0675\n",
            "Epoch 17 Batch 200 Loss 0.0588\n",
            "Epoch 17 Batch 300 Loss 0.0543\n",
            "Epoch 17 Batch 400 Loss 0.0591\n",
            "Epoch 17 Batch 500 Loss 0.0743\n",
            "Epoch 17 Batch 600 Loss 0.0873\n",
            "Epoch 17 Loss 0.0633\n",
            "Time taken for 1 epoch 62.46038293838501 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0560\n",
            "Epoch 18 Batch 100 Loss 0.0328\n",
            "Epoch 18 Batch 200 Loss 0.0526\n",
            "Epoch 18 Batch 300 Loss 0.0569\n",
            "Epoch 18 Batch 400 Loss 0.0525\n",
            "Epoch 18 Batch 500 Loss 0.0606\n",
            "Epoch 18 Batch 600 Loss 0.0716\n",
            "Epoch 18 Loss 0.0584\n",
            "Time taken for 1 epoch 62.932650089263916 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0360\n",
            "Epoch 19 Batch 100 Loss 0.0444\n",
            "Epoch 19 Batch 200 Loss 0.0462\n",
            "Epoch 19 Batch 300 Loss 0.0799\n",
            "Epoch 19 Batch 400 Loss 0.0699\n",
            "Epoch 19 Batch 500 Loss 0.0588\n",
            "Epoch 19 Batch 600 Loss 0.0575\n",
            "Epoch 19 Loss 0.0540\n",
            "Time taken for 1 epoch 62.40230941772461 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0317\n",
            "Epoch 20 Batch 100 Loss 0.0605\n",
            "Epoch 20 Batch 200 Loss 0.0411\n",
            "Epoch 20 Batch 300 Loss 0.0441\n",
            "Epoch 20 Batch 400 Loss 0.0608\n",
            "Epoch 20 Batch 500 Loss 0.0624\n",
            "Epoch 20 Batch 600 Loss 0.0461\n",
            "Epoch 20 Loss 0.0512\n",
            "Time taken for 1 epoch 62.891151428222656 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjanl2-IUz2o"
      },
      "source": [
        "def predict(inputs):\n",
        "    inputs = tf.convert_to_tensor([inputs])\n",
        "    predicted_seq = []\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input,\n",
        "                                          dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        predicted_seq.append(predicted_id)\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return predicted_seq\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return predicted_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6kO2HY0bEn0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate(sentence):\n",
        "    inputs = [tokenizer_en.word_index[i.lower()] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=len_en,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    predicted_seq = predict(inputs)\n",
        "\n",
        "    result = ' '.join([tokenizer_ja.index_word[i] for i in predicted_seq])\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zIxy2Zf3vw"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIbzsxI-f8Z9",
        "outputId": "36aa1e5e-5067-4db4-9fee-672ebe96beee"
      },
      "source": [
        "# checkpoint_dir の中の最後のチェックポイントを復元\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f12c91e8e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX4evgWVf9xa",
        "outputId": "6b49699a-cd77-46b8-8cf3-86511eef05bc"
      },
      "source": [
        "translate('it is necessary that the bill pass the diet .')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: it is necessary that the bill pass the diet .\n",
            "Predicted translation: その 計画 は うま く い く か も しれ な い 。 <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhAZ9RKv9HlL"
      },
      "source": [
        "def predict(inputs):\n",
        "    inputs = tf.convert_to_tensor([inputs])\n",
        "    predicted_seq = []\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input,\n",
        "                                          dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        predicted_seq.append(predicted_id)\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return predicted_seq\n",
        "\n",
        "        # 予測された ID がモデルに戻される\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return predicted_seq\n",
        "    "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qYv1rYq1Lu5"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import pandas as pd\n",
        "\n",
        "input_sentences = []\n",
        "target_sentences = []\n",
        "predicted_sentences = []\n",
        "\n",
        "for i, input_en in enumerate(input_tensor_val):\n",
        "    predicted_ja = predict(input_en)\n",
        "    tokens_input = [inp_lang.index_word[id] for id in input_en if id > 2]\n",
        "    tokens_target = [targ_lang.index_word[id] for id in target_tensor_val[i] if id > 2]\n",
        "    tokens_predicted = [targ_lang.index_word[id] for id in predicted_ja if id > 2]\n",
        "    input_sentences.append(' '.join(tokens_input))\n",
        "    target_sentences.append(''.join(tokens_target))\n",
        "    predicted_sentences.append(''.join(tokens_predicted))\n",
        "\n",
        "result_df = pd.DataFrame({'input_sentence': input_sentences,\n",
        "                          'target_sentence': target_sentences,\n",
        "                          'predicted_sentence': predicted_sentences})  \n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16qn07BeMCJs"
      },
      "source": [
        "bleu_scores = []\n",
        "for row in result_df.itertuples():\n",
        "    bleu_scores.append(\n",
        "        sentence_bleu(row.target_sentence, row.predicted_sentence,\n",
        "                      smoothing_function=SmoothingFunction().method4)\n",
        "    )\n",
        "result_df['bleu_score'] = bleu_scores"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnwSEkf9HlM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "9ed3d5f6-4579-49b9-c4e1-eee0f13b19c2"
      },
      "source": [
        "result_df"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "      <th>predicted_sentence</th>\n",
              "      <th>bleu_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i was wrong .</td>\n",
              "      <td>私が間違っていました。</td>\n",
              "      <td>私は間違っていた。</td>\n",
              "      <td>0.331186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>she is going to learn how to drive .</td>\n",
              "      <td>彼女は近く、運転を習うつもりでいます。</td>\n",
              "      <td>彼女は車のために勉強する予定だ。</td>\n",
              "      <td>0.279488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am fond of skiing .</td>\n",
              "      <td>私はスキーが好きだ。</td>\n",
              "      <td>私はスキーが好きです。</td>\n",
              "      <td>0.336136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i will start working on july the first .</td>\n",
              "      <td>７月１日から仕事を始めます。</td>\n",
              "      <td>私は二日間の日に発つつもりです。</td>\n",
              "      <td>0.245981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ken took the examination with confidence .</td>\n",
              "      <td>ケンは自信をもって試験を受けた。</td>\n",
              "      <td>ケンは試験を尽くしたが無駄だった。</td>\n",
              "      <td>0.321431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>what does the company produce ?</td>\n",
              "      <td>その会社は何を作っているのですか。</td>\n",
              "      <td>その会社は何をしているのか。</td>\n",
              "      <td>0.353037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>he was absent from school because he was sick .</td>\n",
              "      <td>彼は病気だったので学校を休んだ。</td>\n",
              "      <td>彼は病気で学校を欠席した。</td>\n",
              "      <td>0.339871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>won 't you take a chair ?</td>\n",
              "      <td>座りませんか。</td>\n",
              "      <td>煙草を吸ってくれない？</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>i had nothing to do with that incident .</td>\n",
              "      <td>私はその事件とは何の関係も無かった。</td>\n",
              "      <td>私はその出来事とは何の関係もなかった。</td>\n",
              "      <td>0.354081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>i walked as slowly as i could .</td>\n",
              "      <td>私はできるだけゆっくりと歩きました。</td>\n",
              "      <td>私は、わたしたちより外出した。</td>\n",
              "      <td>0.294664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       input_sentence  ... bleu_score\n",
              "0                                       i was wrong .  ...   0.331186\n",
              "1                she is going to learn how to drive .  ...   0.279488\n",
              "2                               i am fond of skiing .  ...   0.336136\n",
              "3            i will start working on july the first .  ...   0.245981\n",
              "4          ken took the examination with confidence .  ...   0.321431\n",
              "...                                               ...  ...        ...\n",
              "9995                  what does the company produce ?  ...   0.353037\n",
              "9996  he was absent from school because he was sick .  ...   0.339871\n",
              "9997                        won 't you take a chair ?  ...   0.000000\n",
              "9998         i had nothing to do with that incident .  ...   0.354081\n",
              "9999                  i walked as slowly as i could .  ...   0.294664\n",
              "\n",
              "[10000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9hQ_XKxIQ2W",
        "outputId": "3e30de47-f27d-48d9-9cb2-ddb562fbeb71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result_df.bleu_score.mean()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3100502226469779"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STA-iR-rYKEe"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    }
  ]
}