{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "livedoor_news_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DSAkbBir4Q0"
      },
      "source": [
        "# livedoorニュースの分類\n",
        "## MeCabのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ZCsC-kMfIp"
      },
      "source": [
        "!apt install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puPer6GPsDfC"
      },
      "source": [
        "## データファイルのダウンロードと解凍"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBz6-6rOPhZF"
      },
      "source": [
        "import tarfile \n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "FILE_URL = 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
        "FILE_PATH = '/content/ldcc-20140209.tar.gz'\n",
        "EXTRACT_DIR = '/content'\n",
        "\n",
        "urlretrieve(FILE_URL, FILE_PATH)\n",
        "\n",
        "mode = \"r:gz\"\n",
        "tar = tarfile.open(FILE_PATH, mode) \n",
        "tar.extractall(EXTRACT_DIR) \n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBkXQA7TPqFU"
      },
      "source": [
        "!ls -l /content/text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJKB43nMa3U"
      },
      "source": [
        "## 記事ファイルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3nFNgwPMa3W"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_dir = '/content/text'\n",
        "\n",
        "category = pd.Series(name='category')\n",
        "url = pd.Series(name='url')\n",
        "time_published = pd.Series(name='time_published')\n",
        "title = pd.Series(name='title')\n",
        "text = pd.Series(name='text')\n",
        "\n",
        "index = 0\n",
        "\n",
        "for name in os.listdir(base_dir):\n",
        "    if os.path.isdir(os.path.join(base_dir, name)):\n",
        "        for file in os.listdir(os.path.join(base_dir, name)):\n",
        "            if file != 'LICENSE.txt':\n",
        "                with open(os.path.join(base_dir, name, file), 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "                category.at[index] = name\n",
        "                url.at[index] = lines[0].rstrip()\n",
        "                time_published.at[index] = lines[1].rstrip()\n",
        "                title.at[index] = lines[2].rstrip()\n",
        "                text.at[index] = ''.join(lines[3:])\n",
        "                index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwpzF_JMa3b"
      },
      "source": [
        "df = pd.concat([category, url, time_published, title, text], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYebo3NOMa3e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOMYsfkjMa3k"
      },
      "source": [
        "## MeCabによるテキストのトークン化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3fPCu2LMa3m"
      },
      "source": [
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "def tokenize_japanese(text):\n",
        "    return tagger.parse(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vaOeJYdMa3o"
      },
      "source": [
        "df['text'] = df['text'].map(tokenize_japanese)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLrkbwgIMa3r"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhgksFq0Ma3v"
      },
      "source": [
        "## KerasのTokenizerにより単語を整数にエンコード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI4hcXxFMa3w"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "MAX_WORDS = 20000 # 最も頻度の高い20,000語のみエンコード\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
        "\n",
        "tokenizer.fit_on_texts(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOuJkuPxMa3z"
      },
      "source": [
        "df['sequence'] = tokenizer.texts_to_sequences(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXLsS3woMa32"
      },
      "source": [
        "word2int = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiMVueGdMa35"
      },
      "source": [
        "print(df['sequence'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWY83Y6CMa39"
      },
      "source": [
        "## Kerasのpad_sequencesで長さを統一"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHySkGZrMa3_"
      },
      "source": [
        "MAX_LENGTH = 5000\n",
        "\n",
        "x_seq = tf.keras.preprocessing.sequence.pad_sequences(df['sequence'], maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN85y3tBMa4B"
      },
      "source": [
        "## カテゴリーを整数にエンコーディング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PElLjXKwMa4D"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "df['encoded_label'] = le.fit_transform(df['category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFIK3Mc1Ma4F"
      },
      "source": [
        "## （１）テキストをマルチホットエンコーディング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYw5q9VSMa4H"
      },
      "source": [
        "（サンプル数×最大単語数）のオールゼロ行列を準備する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq97qJMoMa4H"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "words_multi_hot = np.zeros((x_seq.shape[0], MAX_WORDS))\n",
        "words_multi_hot.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNBYK4IYMa4N"
      },
      "source": [
        "テキスト中にある単語のコードをインデックスにして１を立てる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlVJ3KDCMa4P"
      },
      "source": [
        "for i in range(x_seq.shape[0]):\n",
        "    words_multi_hot[i,x_seq[i]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FId64MSiMa4W"
      },
      "source": [
        "words_multi_hot[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeDSEb1HMa4g"
      },
      "source": [
        "num_categories = len(np.unique(df['category']))\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "inputs = tf.keras.Input(shape=(20000,))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu', input_shape=(20000,)))\n",
        "model.add(tf.keras.layers.Dense(num_categories, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRCbZEx5Ma4m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(words_multi_hot, df['encoded_label'], test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKYQwY6y1HFi"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ckt7FJ8ya8jq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDu9Y_LrMa4s"
      },
      "source": [
        "## （２）単語埋め込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyJvdX_8Ma4t"
      },
      "source": [
        "emb_dim = 128\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(MAX_WORDS, emb_dim, input_length=MAX_LENGTH))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(num_categories, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi_yNGmHMa4w"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_seq, df['encoded_label'], test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mp9yM1M_QDF"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzYoTdeIMa44"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI9vfcWVT64j"
      },
      "source": [
        "## RNN（LSTM）を使ってみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6-qyPzT-HI"
      },
      "source": [
        "EMB_DIMS = 128\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(MAX_WORDS, EMB_DIMS, input_length=MAX_LENGTH))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.4, noise_shape=(None, 1, EMB_DIMS)))\n",
        "model.add(tf.keras.layers.LSTM(128))\n",
        "model.add(tf.keras.layers.Dense(num_categories, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB76TkDRURBs"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_seq, df['encoded_label'], test_size=0.3)\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=30,\n",
        "                    epochs=20,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJpXWYJs91AD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6oQV7MRPO3N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}