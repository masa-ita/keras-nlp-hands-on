{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seqを使った機械翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSNアイネットの社内LAN用の設定\n",
    "プロキシ配下でtf.keras.utils.get_file()がSSL証明書をロードできないエラーを回避するため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for BSN I-Net local only\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英日翻訳データセットのダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この演習では、小田悠介氏（現：LegalForce Research チーフリサーチャー）が公開している \"small_parallel_enja\" を使用します。  \n",
    "このデータセットは、日英対訳コーパスとして知られている[TANAKA corpus](http://www.edrdg.org/wiki/index.php/Tanaka_Corpus)から、文長が4単語から16単語までの短い文を抽出し、トークナイザ（英語:Stanford Tokenizer/日本語:KyTea）でトークナイズしたデータセットです。  \n",
    "下記のコードでは、`tf.keras.utils.get_file()`を使ってGitHubからZIPファイルをダウンロードして展開し、展開されたディレクトリの内容を確認しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsrpAmZ1BVFF",
    "outputId": "4f86b029-427d-4f1a-e5bc-89a286ec9b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20784\n",
      "-rw-r--r--  1 itagaki  staff     1946  2 12 13:07 README.md\n",
      "-rw-r--r--  1 itagaki  staff    17054  2 12 13:07 dev.en\n",
      "-rw-r--r--  1 itagaki  staff    27781  2 12 13:07 dev.ja\n",
      "-rw-r--r--  1 itagaki  staff    17301  2 12 13:07 test.en\n",
      "-rw-r--r--  1 itagaki  staff    27793  2 12 13:07 test.ja\n",
      "-rw-r--r--  1 itagaki  staff  1701356  2 12 13:07 train.en\n",
      "-rw-r--r--  1 itagaki  staff   339768  2 12 13:07 train.en.000\n",
      "-rw-r--r--  1 itagaki  staff   340186  2 12 13:07 train.en.001\n",
      "-rw-r--r--  1 itagaki  staff   341174  2 12 13:07 train.en.002\n",
      "-rw-r--r--  1 itagaki  staff   339953  2 12 13:07 train.en.003\n",
      "-rw-r--r--  1 itagaki  staff   340275  2 12 13:07 train.en.004\n",
      "-rw-r--r--  1 itagaki  staff    30025  2 12 13:07 train.en.vocab.4k\n",
      "-rw-r--r--  1 itagaki  staff    51162  2 12 13:07 train.en.vocab.all\n",
      "-rw-r--r--  1 itagaki  staff  2784447  2 12 13:07 train.ja\n",
      "-rw-r--r--  1 itagaki  staff   556444  2 12 13:07 train.ja.000\n",
      "-rw-r--r--  1 itagaki  staff   555732  2 12 13:07 train.ja.001\n",
      "-rw-r--r--  1 itagaki  staff   557218  2 12 13:07 train.ja.002\n",
      "-rw-r--r--  1 itagaki  staff   557538  2 12 13:07 train.ja.003\n",
      "-rw-r--r--  1 itagaki  staff   557515  2 12 13:07 train.ja.004\n",
      "-rw-r--r--  1 itagaki  staff    31009  2 12 13:07 train.ja.vocab.4k\n",
      "-rw-r--r--  1 itagaki  staff    73669  2 12 13:07 train.ja.vocab.all\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://github.com/odashi/small_parallel_enja/archive/master.zip'\n",
    "\n",
    "zip_file_path = get_file('small_parallel_enja.zip', URL, cache_subdir='small_parallel_enja', extract=True) \n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(zip_file_path), 'small_parallel_enja-master')\n",
    "\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練用データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small_parallel_enjaデータセットでは、訓練用のデータが50,000ペア含まれていますが、ここでは学習にかかる時間を短縮するため、最初の30000件のみを使用します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i can 't tell who will arrive first .\", 'many animals have been destroyed by men .', \"i 'm in the tennis club .\"]\n",
      "['誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。', '多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。', '私 は テニス 部員 で す 。']\n"
     ]
    }
   ],
   "source": [
    "# 訓練時間節約のためデータの一部のみ使用する\n",
    "TRAIN_SIZE_LIMIT = 30000\n",
    "\n",
    "def load_data(path):\n",
    "    texts = []\n",
    "    for line in open(path, 'r'):\n",
    "        texts.append(line.strip()) # 改行コードなどを除去\n",
    "    return texts\n",
    "  \n",
    "\n",
    "train_en = load_data(os.path.join(data_dir, 'train.en'))\n",
    "train_ja = load_data(os.path.join(data_dir, 'train.ja'))\n",
    "\n",
    "train_en = train_en[:TRAIN_SIZE_LIMIT]\n",
    "train_ja = train_ja[:TRAIN_SIZE_LIMIT]\n",
    "\n",
    "print(train_en[:3])\n",
    "print(train_ja[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの前処理を行います。今回のデータはトークナイズ済みであるため、ユニコードの表現のブレを正規化する処理と、文頭及び文末にそれぞれ目印となるトークンを追加する処理のみを行なっています。データや目的によっては、記号やHTMLタグなどの除去や大文字の小文字への変換などを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = unicodedata.normalize('NFKC', text) # ユニコード正規化\n",
    "    text = '<start> ' + text + ' <end>'        # 文頭と文末を表すトークンを追加\n",
    "    return text\n",
    "\n",
    "train_input = [preprocess(s) for s in train_en]\n",
    "train_target = [preprocess(s) for s in train_ja]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力および出力の最大長"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータセットでは、英語、日本語とも最大長は16語、これに<start>, <end>を加えた18が最大長です。 　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input max length: 18\n",
      "target max length: 18\n"
     ]
    }
   ],
   "source": [
    "input_lengths = [len(s.split(' ')) for s in train_input]\n",
    "target_lengths = [len(s.split(' ')) for s in train_target]\n",
    "\n",
    "max_length_inp = max(input_lengths)\n",
    "max_length_targ = max(target_lengths)\n",
    "\n",
    "print('input max length:', max_length_inp)\n",
    "print('target max length:', max_length_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークンのベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークンを機械学習モデルで扱えるようにベクトル化します。  \n",
    "TensorFlowで使えるベクトル化モジュールには、`tf.keras.preprocessing.text.Tokenizer`などいくつかありますが、ここでは新しく開発された`tf.keras.layers.experimental.preprocessing.TextVectorization`を使用します。このクラスは、レイヤとして実装されているため、関数として使用する他に、モデルに組み込んで実行することも可能です。また、出力も今回使用するトークンを整数値にする'int'モードの他に、テキスト全体をマルチホットベクトル化する'binary'モード、単語の出現回数のベクトルにする'count'モード、TF-IDFベクトル化する'tf-idf'モードが使える他、出力の長さを揃えてパディングを行う機能も備えています。\n",
    "\n",
    "TextVectorization()では、ボキャブラリーをファイルから直接読み込ませることもできますが、今回は訓練用データをつかって語彙の抽出を行なっています。\n",
    "\n",
    "ベクトル化したデータは、tf.data.Datasetに変換し、input（英語）とtarget（日本語）のペアを構成するtrain_dsデータセットに加工しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vectorizer = TextVectorization(max_tokens=20000,\n",
    "                                  standardize=None,\n",
    "                                  output_mode='int',\n",
    "                                  output_sequence_length=max_length_inp)\n",
    "\n",
    "ja_vectorizer = TextVectorization(max_tokens=20000,\n",
    "                                  standardize=None,\n",
    "                                  output_mode='int',\n",
    "                                  output_sequence_length=max_length_targ)\n",
    "\n",
    "en_vectorizer.adapt(train_input)\n",
    "ja_vectorizer.adapt(train_target)\n",
    "\n",
    "train_input_ds = tf.data.Dataset.from_tensor_slices(en_vectorizer(train_input))\n",
    "train_target_ds = tf.data.Dataset.from_tensor_slices(ja_vectorizer(train_target))\n",
    "\n",
    "train_ds = tf.data.Dataset.zip((train_input_ds, train_target_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ボキャブラリの抽出と語彙数の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextVectorizationでは、パディングに使われる0も空文字列としてボキャブラリに含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '<start>', '<end>', '.', 'the', 'i', 'to', 'you', 'is']\n",
      "['', '[UNK]', '<start>', '<end>', '。', 'は', 'い', 'に', 'た', 'を']\n"
     ]
    }
   ],
   "source": [
    "en_vocab = en_vectorizer.get_vocabulary()\n",
    "ja_vocab = ja_vectorizer.get_vocabulary()\n",
    "vocab_inp_size = len(en_vocab)\n",
    "vocab_tar_size = len(ja_vocab)\n",
    "\n",
    "print(en_vocab[:10])\n",
    "print(ja_vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルデータの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20969,
     "status": "ok",
     "timestamp": 1612533622233,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "cfxSreRrXvw4"
   },
   "outputs": [],
   "source": [
    "def convert(vocab, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, vocab[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, ja = next(iter(train_ds)) # train_dsから最初のデータを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20946,
     "status": "ok",
     "timestamp": 1612533622233,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "qBMUz4Q9Xxe7",
    "outputId": "dae0dde9-4b87-4f04-f9c0-83406ba9bfbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "6 ----> i\n",
      "41 ----> can\n",
      "22 ----> 't\n",
      "149 ----> tell\n",
      "136 ----> who\n",
      "29 ----> will\n",
      "709 ----> arrive\n",
      "231 ----> first\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "92 ----> 誰\n",
      "14 ----> が\n",
      "239 ----> 一番\n",
      "7 ----> に\n",
      "161 ----> 着\n",
      "29 ----> く\n",
      "22 ----> か\n",
      "18 ----> 私\n",
      "7 ----> に\n",
      "5 ----> は\n",
      "290 ----> 分か\n",
      "39 ----> り\n",
      "21 ----> ま\n",
      "40 ----> せ\n",
      "30 ----> ん\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(en_vocab, en.numpy())\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(ja_vocab, ja.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seqモデルは下図のような構造を持っています。この例では機械翻訳ではなく、会話ボットを想定しています。\n",
    "\n",
    "リカレントニューラルネットワーク（RNN）を使ったEncoderで入力文字列を処理し、最後の状態をDecoderに渡します。\n",
    "\n",
    "Decoderはこの状態行列と、文頭を表すトークン<start>を入力として、次に来る文字をRNNを使って予測します。次に予測したトークンと更新された状態を使って次の文字を予測し、これを文末トークン<end>が予測されるか、最大長に達するまで繰り返します。\n",
    "\n",
    "![seq2seq model](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
    "\n",
    "from \"Write a Sequence to Sequence (seq2seq) Model -- Chainer\" https://docs.chainer.org/en/stable/examples/seq2seq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築に必要なパラメータ定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 21908,
     "status": "ok",
     "timestamp": 1612533623222,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "ZDYSOGLzYbP8"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_input)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(train_input) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練用データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset`では、データのシャッフル`shuffle`やバッチ化`batch`を使ったメソッドチェインでデータの加工が可能です。  \n",
    "サイズの大きなデータセットの場合、`prefetch`や`cache`を行うことで、高速化も期待できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 21908,
     "status": "ok",
     "timestamp": 1612533623222,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "ZDYSOGLzYbP8"
   },
   "outputs": [],
   "source": [
    "dataset = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力データのshapeの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27557,
     "status": "ok",
     "timestamp": 1612531066914,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "OkZvi19gYp03",
    "outputId": "f0d807c6-9df8-44e2-9752-8d56e48e416f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 18]), TensorShape([64, 18]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、`Embedding`レイヤと`GRU`レイヤを1つずつ使ったシンプルなエンコーダを定義します。  \n",
    "`tf.keras`を使ったモデル定義の方法としては、`tf.keras.Sequential`を使う方法や、レイヤの数珠つなぎを自分で作成するFunctional APIがありますが、ここでは`tf.keras.Model`を継承したサブクラス化の手法を使用しています。\n",
    "サブクラスでのモデル定義では、レイヤなどの定義を初期化メソッド（ __init__ ）でインスタンス変数を使って行い、クラスを関数として呼び出すためのcallメソッドで、順方向のデータ処理の流れを定義します。\n",
    "\n",
    "\n",
    "リカレントニューラルネットワークでよく使われるのは`LSTM`ですが、ここではこれを簡略化した`GRU`を使うことで、訓練に要する時間を節約します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 27534,
     "status": "ok",
     "timestamp": 1612531066916,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "YYHYgc4hYwly"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderの出力shapeの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28496,
     "status": "ok",
     "timestamp": 1612531067899,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "v5fs_ECmZP8N",
    "outputId": "bd244cb8-b367-4e29-8c67-3c04a4c8746c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, units) (64, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# サンプル入力\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoderの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 28472,
     "status": "ok",
     "timestamp": 1612531067900,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "yzuNjW0lZs1y"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Embeddingの出力と、エンコーダ出力を GRU 層に渡す\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoderの出力shapeの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28452,
     "status": "ok",
     "timestamp": 1612531067901,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "yP8fSgqDZ9Ci",
    "outputId": "e281a1bf-a725-4e53-c528-fc7d26396cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 6951)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderの出力は多クラス分類であるため、損失関数には`tf.keras.losses.SparseCategoricalCrossentropy`を使います。この例ではDecoder内部でsoftmax変換をかけていないので、`from_logit=True`に設定しています。また、バッチごとの損失をそのまま受け取りたいので、`reduction='none'`に設定します。\n",
    "\n",
    "\n",
    "パディングされたデータで正しく損失値を計算するために、パディングからmaskを計算して計算から除外しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 28428,
     "status": "ok",
     "timestamp": 1612531067902,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "xOmW4oGQaBuJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                             from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェックポイントの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 28424,
     "status": "ok",
     "timestamp": 1612531067903,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "j1xOnT6oaOCY"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練の1ステップを定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力データのバッチ1つずつに対応する`train_step()`関数を定義します。  \n",
    "勾配計算を行うために、`with tf.GradientTape() as tape`のコンテキストでバッチの順方向処理を行います。  \n",
    "訓練の効率を上げるために、前ステップでDecoderが予測したトークンではなく、教師データ（target）の該当位置のトークンを使用しています。これをTeacher Forcingと呼びます。  \n",
    "順方向処理が終わったら、`tape`を使って損失の訓練可能パラメータに対する勾配を計算し、計算された勾配をオプティマイザに渡してパラメータを更新します。\n",
    "\n",
    "この関数は`@tf.function`でアノテートし、TensorFlowの計算グラフ中で計算できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 28420,
     "status": "ok",
     "timestamp": 1612531067904,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "DaknH7tgaTEt"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([ja_vocab.index('<start>')] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher Forcing - 正解値を次の入力として供給\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Teacher Forcing を使用\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練ループ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練用データを1バッチずつ取り出し、上で定義した`train_step()`関数を使ってバッチごとの損失からエポックごとの損失を計算して経過時間とともに表示します。  \n",
    "この例では、100バッチごとに損失を表示し、また、2エポックごとにチェックポイントの保存を行なっています。  \n",
    "validationデータを使用する場合には、エポックごと、あるいは定義したエポック感覚でvalidation lossを計算することもできます。その際には、勾配計算とパラメータ更新は行なってはいけません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4281824,
     "status": "ok",
     "timestamp": 1612535321315,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "5bNwa8eCa0Ru",
    "outputId": "dd0bf040-9033-4d79-c3f1-c3c6907ac304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.3353\n",
      "Epoch 1 Batch 100 Loss 2.6716\n",
      "Epoch 1 Batch 200 Loss 2.3507\n",
      "Epoch 1 Batch 300 Loss 1.9240\n",
      "Epoch 1 Batch 400 Loss 1.9716\n",
      "Epoch 1 Loss 2.3301\n",
      "Time taken for 1 epoch 505.9071218967438 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練済みモデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練済みのモデルを使って実際に翻訳を実行し、その性能を見てみたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価用関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理を行なっていない英文を受け取り、翻訳モデルでの翻訳結果（分かち書き）を返す`evaluate()`関数を定義します。  \n",
    "モデルは、バッチデータを対象としていることに注意が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1986744,
     "status": "ok",
     "timestamp": 1612535588224,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "H6kO2HY0bEn0"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    \n",
    "    inputs = en_vectorizer([sentence])\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([ja_vocab.index('<start>')], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input,\n",
    "                                          dec_hidden)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        if ja_vocab[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "        result += ja_vocab[predicted_id] + ' '\n",
    "\n",
    "        # 予測された ID がモデルに戻される\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文の入力を翻訳し、翻訳結果とともに表示する関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "q-zIxy2Zf3vw"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後のチェックポイントをencoderとdecorderにロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIbzsxI-f8Z9",
    "outputId": "36aa1e5e-5067-4db4-9fee-672ebe96beee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x16860e750>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir の中の最後のチェックポイントを復元\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "executionInfo": {
     "elapsed": 1987264,
     "status": "ok",
     "timestamp": 1612535588788,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "aX4evgWVf9xa",
    "outputId": "831a9943-b7d0-4458-c854-0203c14ea490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i am a little out of sorts today . <end>\n",
      "Predicted translation: 私 は その ニュース を 聞 い て い る 。 \n"
     ]
    }
   ],
   "source": [
    "translate(\"i am a little out of sorts today .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト用データセットを使った評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時間節約のため、テスト用データセットの先頭300件を対象として、訓練済みモデルによる翻訳を行い、正解（翻訳例）との比較を行うためのデータフレームを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9qYv1rYq1Lu5"
   },
   "outputs": [],
   "source": [
    "# 時間節約のため、最初の300件のみを使用する。\n",
    "TEST_SIZE_LIMIT = 300\n",
    "\n",
    "input_sentences = load_data(os.path.join(data_dir, 'test.en'))\n",
    "target_sentences = load_data(os.path.join(data_dir, 'test.ja'))\n",
    "\n",
    "input_sentences = input_sentences[:TEST_SIZE_LIMIT]\n",
    "target_sentences = target_sentences[:TEST_SIZE_LIMIT]\n",
    "predicted_sentences = []\n",
    "\n",
    "for input_en in input_sentences:\n",
    "    predicted_ja, _ = evaluate(preprocess(input_en))\n",
    "    predicted_sentences.append(''.join(predicted_ja[:-1]))\n",
    "\n",
    "result_df = pd.DataFrame({'input_sentence': input_sentences,\n",
    "                          'target_sentence': target_sentences,\n",
    "                          'predicted_sentence': predicted_sentences})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEUスコアによる評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械翻訳のように、テキスト生成で正解あるいはお手本となるテキストにどれだけ近い出力ができたかを評価する指標としてBLUEスコアがあります。\n",
    "\n",
    "BLEUスコアの定義は下記の通りで、基本はN-グラム単位での一致がどれくらいあるかです。\n",
    "\n",
    "$$\n",
    "    BLEU = BP_{BLEU} \\times \\exp(\\sum_{i=0}^Nw_n\\log p_n) \\\\\n",
    "    p_n = \\frac{\\sum_i翻訳文iと参照訳iで一致したn–gram数}{\\sum_i翻訳文i中の全n–gram数} \\\\\n",
    "    w_n = \\frac{1}{N} \\\\\n",
    "    BP_{BLEU} = \n",
    "        \\begin{cases}\n",
    "            1 \\quad c \\geqq r \\\\\n",
    "            \\exp(1-\\frac{r}{c}) \\quad c < r \\\\\n",
    "        \\end{cases} \\\\\n",
    "        ただし、\n",
    "        \\begin{cases}\n",
    "        c = \\sum_i 翻訳文の長さ \\\\ \n",
    "        r = \\sum_{参照訳集合} 参照訳中で対応する翻訳文に最も近い長さ \\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "ペナルティ $BP_{BLEU}$ は、短い翻訳文の一致度が高くなることに対する対策です。\n",
    "\n",
    "ただし、Nが大きくなると一致度が低下し、スコアが急激に低下する問題があるため、これを平滑化する手法がいくつも提案されています。  \n",
    "\n",
    "今回は、nltkライブラリの`nltk.translate.blue_score.sentence_bleu()`を使用して計算していますが、このライブラリでmethod4として実装されている平滑化手法を使用しています。\n",
    "\n",
    "下記のコードでBLEU値を計算し、データフレームに列を追加しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "16qn07BeMCJs"
   },
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "for row in result_df.itertuples():\n",
    "    bleu_scores.append(\n",
    "        sentence_bleu(row.target_sentence, row.predicted_sentence,\n",
    "                      smoothing_function=SmoothingFunction().method4)\n",
    "    )\n",
    "result_df['bleu_score'] = bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "bGnwSEkf9HlM",
    "outputId": "9ed3d5f6-4579-49b9-c4e1-eee0f13b19c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "      <th>predicted_sentence</th>\n",
       "      <th>bleu_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they finally acknowledged it as true .</td>\n",
       "      <td>彼 ら は つい に それ が 真実 だ と 認め た 。</td>\n",
       "      <td>彼 ら は 彼 が 話 を する の を 許 し た 。</td>\n",
       "      <td>0.153911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he didn 't care for swimming .</td>\n",
       "      <td>彼 は 水泳 が 得意 で は な かっ た 。</td>\n",
       "      <td>彼 は 、 以前 、 彼 の こと に 賛成 し な かっ た 。</td>\n",
       "      <td>0.131693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he is no less kind than his sister .</td>\n",
       "      <td>彼 は お 姉 さん に 劣 ら ず 親切 だ 。</td>\n",
       "      <td>彼 は お 姉 さん ほど 背 が 高 く な い 。</td>\n",
       "      <td>0.166010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you must be back before ten .</td>\n",
       "      <td>１０ 時 前 に 戻 ら な けれ ば な ら な い 。</td>\n",
       "      <td>10 分 も か ら ず に つ か な けれ ば な ら な い 。</td>\n",
       "      <td>0.126820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>break a leg .</td>\n",
       "      <td>成功 を 祈 る わ 。</td>\n",
       "      <td>時 がいつ し か 過ぎ て 。</td>\n",
       "      <td>0.220057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>i 'm very sleepy now .</td>\n",
       "      <td>今 とても 眠 い 。</td>\n",
       "      <td>今 とても 忙し そう だ 。</td>\n",
       "      <td>0.313971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>above the music , i could hear her crying .</td>\n",
       "      <td>音楽 が な っ て い る の に 彼女 の 鳴き声 が 聞こえ た 。</td>\n",
       "      <td>私 は 音楽 を 聴 く ため に 彼女 の 夢 を 見 ま せ ん 。</td>\n",
       "      <td>0.122821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>we had the meeting in this room last friday .</td>\n",
       "      <td>先週 の 金曜 日 この 部屋 で 会合 が あ っ た 。</td>\n",
       "      <td>この バス で 私 たち は 昨日 、 交通 事故 に あ っ た 。</td>\n",
       "      <td>0.126820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>who do you want to speak to ?</td>\n",
       "      <td>お 話 に な る 方 の お 名前 は 。</td>\n",
       "      <td>あなた は 話 す べ き もの で は な い の か 。</td>\n",
       "      <td>0.136731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>his father eats there twice a week .</td>\n",
       "      <td>彼 の 父 は １ 週間 に ２ 回 そこ で 食べ る 。</td>\n",
       "      <td>父 が 一 週間 に 2、3 の 雨 が あ る ら しい 。</td>\n",
       "      <td>0.141483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    input_sentence  \\\n",
       "0           they finally acknowledged it as true .   \n",
       "1                   he didn 't care for swimming .   \n",
       "2             he is no less kind than his sister .   \n",
       "3                    you must be back before ten .   \n",
       "4                                    break a leg .   \n",
       "..                                             ...   \n",
       "295                         i 'm very sleepy now .   \n",
       "296    above the music , i could hear her crying .   \n",
       "297  we had the meeting in this room last friday .   \n",
       "298                  who do you want to speak to ?   \n",
       "299           his father eats there twice a week .   \n",
       "\n",
       "                           target_sentence  \\\n",
       "0            彼 ら は つい に それ が 真実 だ と 認め た 。   \n",
       "1                 彼 は 水泳 が 得意 で は な かっ た 。   \n",
       "2                彼 は お 姉 さん に 劣 ら ず 親切 だ 。   \n",
       "3            １０ 時 前 に 戻 ら な けれ ば な ら な い 。   \n",
       "4                             成功 を 祈 る わ 。   \n",
       "..                                     ...   \n",
       "295                            今 とても 眠 い 。   \n",
       "296  音楽 が な っ て い る の に 彼女 の 鳴き声 が 聞こえ た 。   \n",
       "297         先週 の 金曜 日 この 部屋 で 会合 が あ っ た 。   \n",
       "298                 お 話 に な る 方 の お 名前 は 。   \n",
       "299         彼 の 父 は １ 週間 に ２ 回 そこ で 食べ る 。   \n",
       "\n",
       "                       predicted_sentence  bleu_score  \n",
       "0            彼 ら は 彼 が 話 を する の を 許 し た 。    0.153911  \n",
       "1       彼 は 、 以前 、 彼 の こと に 賛成 し な かっ た 。    0.131693  \n",
       "2             彼 は お 姉 さん ほど 背 が 高 く な い 。    0.166010  \n",
       "3     10 分 も か ら ず に つ か な けれ ば な ら な い 。    0.126820  \n",
       "4                        時 がいつ し か 過ぎ て 。    0.220057  \n",
       "..                                    ...         ...  \n",
       "295                       今 とても 忙し そう だ 。    0.313971  \n",
       "296  私 は 音楽 を 聴 く ため に 彼女 の 夢 を 見 ま せ ん 。    0.122821  \n",
       "297   この バス で 私 たち は 昨日 、 交通 事故 に あ っ た 。    0.126820  \n",
       "298        あなた は 話 す べ き もの で は な い の か 。    0.136731  \n",
       "299       父 が 一 週間 に 2、3 の 雨 が あ る ら しい 。    0.141483  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9hQ_XKxIQ2W",
    "outputId": "3e30de47-f27d-48d9-9cb2-ddb562fbeb71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17331228745184793"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.bleu_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMVDvrdcVx8hEVViWJnjayE",
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": [
    {
     "file_id": "1DsM1Q65HCjdIloBDCnwGeO3xTxut581q",
     "timestamp": 1612529211758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
