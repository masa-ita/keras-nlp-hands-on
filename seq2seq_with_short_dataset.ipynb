{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_M-eDSyL-8E7",
    "outputId": "7f42f59e-e667-4f23-fa16-57d82dd3e4aa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "url = 'https://github.com/odashi/small_parallel_enja/archive/master.zip'\n",
    "\n",
    "zip_file_path = get_file('small_parallel_enja.zip', url, cache_subdir='small_parallel_enja', extract=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsrpAmZ1BVFF",
    "outputId": "4f86b029-427d-4f1a-e5bc-89a286ec9b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9076\n",
      "-rw-r--r-- 1 tensorflow tensorflow    1946 Feb 11 09:59 README.md\n",
      "-rw-r--r-- 1 tensorflow tensorflow   17054 Feb 11 09:59 dev.en\n",
      "-rw-r--r-- 1 tensorflow tensorflow   27781 Feb 11 09:59 dev.ja\n",
      "-rw-r--r-- 1 tensorflow tensorflow   17301 Feb 11 09:59 test.en\n",
      "-rw-r--r-- 1 tensorflow tensorflow   27793 Feb 11 09:59 test.ja\n",
      "-rw-r--r-- 1 tensorflow tensorflow 1701356 Feb 11 09:59 train.en\n",
      "-rw-r--r-- 1 tensorflow tensorflow  339768 Feb 11 09:59 train.en.000\n",
      "-rw-r--r-- 1 tensorflow tensorflow  340186 Feb 11 09:59 train.en.001\n",
      "-rw-r--r-- 1 tensorflow tensorflow  341174 Feb 11 09:59 train.en.002\n",
      "-rw-r--r-- 1 tensorflow tensorflow  339953 Feb 11 09:59 train.en.003\n",
      "-rw-r--r-- 1 tensorflow tensorflow  340275 Feb 11 09:59 train.en.004\n",
      "-rw-r--r-- 1 tensorflow tensorflow   30025 Feb 11 09:59 train.en.vocab.4k\n",
      "-rw-r--r-- 1 tensorflow tensorflow   51162 Feb 11 09:59 train.en.vocab.all\n",
      "-rw-r--r-- 1 tensorflow tensorflow 2784447 Feb 11 09:59 train.ja\n",
      "-rw-r--r-- 1 tensorflow tensorflow  556444 Feb 11 09:59 train.ja.000\n",
      "-rw-r--r-- 1 tensorflow tensorflow  555732 Feb 11 09:59 train.ja.001\n",
      "-rw-r--r-- 1 tensorflow tensorflow  557218 Feb 11 09:59 train.ja.002\n",
      "-rw-r--r-- 1 tensorflow tensorflow  557538 Feb 11 09:59 train.ja.003\n",
      "-rw-r--r-- 1 tensorflow tensorflow  557515 Feb 11 09:59 train.ja.004\n",
      "-rw-r--r-- 1 tensorflow tensorflow   31009 Feb 11 09:59 train.ja.vocab.4k\n",
      "-rw-r--r-- 1 tensorflow tensorflow   73669 Feb 11 09:59 train.ja.vocab.all\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(zip_file_path), 'small_parallel_enja-master')\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "TRAIN_SIZE_LIMIT = 30000\n",
    "TEST_SIZE_LIMIT = 300\n",
    "\n",
    "def load_data(path):\n",
    "    texts = []\n",
    "    for line in open(path, 'r'):\n",
    "        texts.append(line.strip())\n",
    "    return texts\n",
    "  \n",
    "def preprocess(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = '<start> ' + text + ' <end>'\n",
    "    return text\n",
    "\n",
    "train_en = load_data(os.path.join(data_dir, 'train.en'))\n",
    "train_ja = load_data(os.path.join(data_dir, 'train.ja'))\n",
    "\n",
    "train_en = train_en[:TRAIN_SIZE_LIMIT]\n",
    "train_ja = train_ja[:TRAIN_SIZE_LIMIT]\n",
    "\n",
    "train_input = [preprocess(s) for s in train_en]\n",
    "train_target = [preprocess(s) for s in train_ja]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vectorizer = TextVectorization(max_tokens=20000,\n",
    "                                  standardize=None,\n",
    "                                  output_mode='int',\n",
    "                                  output_sequence_length=18)\n",
    "\n",
    "en_vectorizer.adapt(train_input)\n",
    "\n",
    "ja_vectorizer = TextVectorization(max_tokens=20000,\n",
    "                                  standardize=None,\n",
    "                                  output_mode='int',\n",
    "                                  output_sequence_length=18)\n",
    "\n",
    "ja_vectorizer.adapt(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ds = tf.data.Dataset.from_tensor_slices(en_vectorizer(train_input))\n",
    "train_target_ds = tf.data.Dataset.from_tensor_slices(ja_vectorizer(train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip((train_input_ds, train_target_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [],
   "source": [
    "en_vocab = en_vectorizer.get_vocabulary()\n",
    "ja_vocab = ja_vectorizer.get_vocabulary()\n",
    "vocab_inp_size = len(en_vocab) + 1\n",
    "vocab_tar_size = len(ja_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y3iF78mDMb1B"
   },
   "outputs": [],
   "source": [
    "max_length_inp = 18\n",
    "max_length_targ = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 20969,
     "status": "ok",
     "timestamp": 1612533622233,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "cfxSreRrXvw4"
   },
   "outputs": [],
   "source": [
    "def convert(vocab, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, vocab[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, ja = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20946,
     "status": "ok",
     "timestamp": 1612533622233,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "qBMUz4Q9Xxe7",
    "outputId": "dae0dde9-4b87-4f04-f9c0-83406ba9bfbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "6 ----> i\n",
      "41 ----> can\n",
      "22 ----> 't\n",
      "149 ----> tell\n",
      "136 ----> who\n",
      "29 ----> will\n",
      "709 ----> arrive\n",
      "231 ----> first\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "92 ----> 誰\n",
      "14 ----> が\n",
      "239 ----> 一番\n",
      "7 ----> に\n",
      "161 ----> 着\n",
      "29 ----> く\n",
      "22 ----> か\n",
      "18 ----> 私\n",
      "7 ----> に\n",
      "5 ----> は\n",
      "290 ----> 分か\n",
      "39 ----> り\n",
      "21 ----> ま\n",
      "40 ----> せ\n",
      "30 ----> ん\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(en_vocab, en.numpy())\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(ja_vocab, ja.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 21908,
     "status": "ok",
     "timestamp": 1612533623222,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "ZDYSOGLzYbP8"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_input)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(train_input) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "dataset = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27557,
     "status": "ok",
     "timestamp": 1612531066914,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "OkZvi19gYp03",
    "outputId": "f0d807c6-9df8-44e2-9752-8d56e48e416f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 18]), TensorShape([64, 18]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 27534,
     "status": "ok",
     "timestamp": 1612531066916,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "YYHYgc4hYwly"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28496,
     "status": "ok",
     "timestamp": 1612531067899,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "v5fs_ECmZP8N",
    "outputId": "bd244cb8-b367-4e29-8c67-3c04a4c8746c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# サンプル入力\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 28472,
     "status": "ok",
     "timestamp": 1612531067900,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "yzuNjW0lZs1y"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Embeddingの出力と、エンコーダ出力を GRU 層に渡す\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28452,
     "status": "ok",
     "timestamp": 1612531067901,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "yP8fSgqDZ9Ci",
    "outputId": "e281a1bf-a725-4e53-c528-fc7d26396cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 6952)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 28428,
     "status": "ok",
     "timestamp": 1612531067902,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "xOmW4oGQaBuJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                             from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 28424,
     "status": "ok",
     "timestamp": 1612531067903,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "j1xOnT6oaOCY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 28420,
     "status": "ok",
     "timestamp": 1612531067904,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "DaknH7tgaTEt"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([ja_vocab.index('<start>')] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher Forcing - 正解値を次の入力として供給\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Teacher Forcing を使用\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4281824,
     "status": "ok",
     "timestamp": 1612535321315,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "5bNwa8eCa0Ru",
    "outputId": "dd0bf040-9033-4d79-c3f1-c3c6907ac304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.8900\n",
      "Epoch 1 Batch 100 Loss 2.6323\n",
      "Epoch 1 Batch 200 Loss 2.1955\n",
      "Epoch 1 Batch 300 Loss 2.0682\n",
      "Epoch 1 Batch 400 Loss 2.1204\n",
      "Epoch 1 Loss 2.3380\n",
      "Time taken for 1 epoch 41.62293243408203 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7043\n",
      "Epoch 2 Batch 100 Loss 1.6932\n",
      "Epoch 2 Batch 200 Loss 1.6036\n",
      "Epoch 2 Batch 300 Loss 1.8222\n",
      "Epoch 2 Batch 400 Loss 1.6237\n",
      "Epoch 2 Loss 1.7050\n",
      "Time taken for 1 epoch 25.299636125564575 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.4698\n",
      "Epoch 3 Batch 100 Loss 1.4758\n",
      "Epoch 3 Batch 200 Loss 1.4239\n",
      "Epoch 3 Batch 300 Loss 1.4386\n",
      "Epoch 3 Batch 400 Loss 1.4671\n",
      "Epoch 3 Loss 1.4174\n",
      "Time taken for 1 epoch 24.931610822677612 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2043\n",
      "Epoch 4 Batch 100 Loss 1.1332\n",
      "Epoch 4 Batch 200 Loss 1.1463\n",
      "Epoch 4 Batch 300 Loss 1.1626\n",
      "Epoch 4 Batch 400 Loss 1.0412\n",
      "Epoch 4 Loss 1.1670\n",
      "Time taken for 1 epoch 25.572669744491577 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.8723\n",
      "Epoch 5 Batch 100 Loss 0.8590\n",
      "Epoch 5 Batch 200 Loss 0.9150\n",
      "Epoch 5 Batch 300 Loss 0.8782\n",
      "Epoch 5 Batch 400 Loss 0.9644\n",
      "Epoch 5 Loss 0.9399\n",
      "Time taken for 1 epoch 25.135027408599854 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6498\n",
      "Epoch 6 Batch 100 Loss 0.8038\n",
      "Epoch 6 Batch 200 Loss 0.7923\n",
      "Epoch 6 Batch 300 Loss 0.7469\n",
      "Epoch 6 Batch 400 Loss 0.7244\n",
      "Epoch 6 Loss 0.7414\n",
      "Time taken for 1 epoch 25.2159686088562 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4727\n",
      "Epoch 7 Batch 100 Loss 0.5775\n",
      "Epoch 7 Batch 200 Loss 0.5707\n",
      "Epoch 7 Batch 300 Loss 0.5902\n",
      "Epoch 7 Batch 400 Loss 0.5871\n",
      "Epoch 7 Loss 0.5699\n",
      "Time taken for 1 epoch 25.249279975891113 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.3488\n",
      "Epoch 8 Batch 100 Loss 0.3808\n",
      "Epoch 8 Batch 200 Loss 0.4667\n",
      "Epoch 8 Batch 300 Loss 0.3915\n",
      "Epoch 8 Batch 400 Loss 0.4988\n",
      "Epoch 8 Loss 0.4288\n",
      "Time taken for 1 epoch 25.760921716690063 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2720\n",
      "Epoch 9 Batch 100 Loss 0.3322\n",
      "Epoch 9 Batch 200 Loss 0.2918\n",
      "Epoch 9 Batch 300 Loss 0.3279\n",
      "Epoch 9 Batch 400 Loss 0.3569\n",
      "Epoch 9 Loss 0.3156\n",
      "Time taken for 1 epoch 25.1646089553833 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2144\n",
      "Epoch 10 Batch 100 Loss 0.1898\n",
      "Epoch 10 Batch 200 Loss 0.2133\n",
      "Epoch 10 Batch 300 Loss 0.2174\n",
      "Epoch 10 Batch 400 Loss 0.2548\n",
      "Epoch 10 Loss 0.2307\n",
      "Time taken for 1 epoch 25.678743362426758 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.1531\n",
      "Epoch 11 Batch 100 Loss 0.1618\n",
      "Epoch 11 Batch 200 Loss 0.1898\n",
      "Epoch 11 Batch 300 Loss 0.1551\n",
      "Epoch 11 Batch 400 Loss 0.2096\n",
      "Epoch 11 Loss 0.1702\n",
      "Time taken for 1 epoch 25.34059166908264 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.1239\n",
      "Epoch 12 Batch 100 Loss 0.1092\n",
      "Epoch 12 Batch 200 Loss 0.1189\n",
      "Epoch 12 Batch 300 Loss 0.1283\n",
      "Epoch 12 Batch 400 Loss 0.1606\n",
      "Epoch 12 Loss 0.1288\n",
      "Time taken for 1 epoch 25.84565258026123 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0885\n",
      "Epoch 13 Batch 100 Loss 0.0662\n",
      "Epoch 13 Batch 200 Loss 0.1179\n",
      "Epoch 13 Batch 300 Loss 0.0841\n",
      "Epoch 13 Batch 400 Loss 0.1108\n",
      "Epoch 13 Loss 0.1047\n",
      "Time taken for 1 epoch 25.228084564208984 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0709\n",
      "Epoch 14 Batch 100 Loss 0.0832\n",
      "Epoch 14 Batch 200 Loss 0.0830\n",
      "Epoch 14 Batch 300 Loss 0.1087\n",
      "Epoch 14 Batch 400 Loss 0.1166\n",
      "Epoch 14 Loss 0.0949\n",
      "Time taken for 1 epoch 25.716827869415283 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0751\n",
      "Epoch 15 Batch 100 Loss 0.0671\n",
      "Epoch 15 Batch 200 Loss 0.0777\n",
      "Epoch 15 Batch 300 Loss 0.0929\n",
      "Epoch 15 Batch 400 Loss 0.1197\n",
      "Epoch 15 Loss 0.0970\n",
      "Time taken for 1 epoch 25.46717405319214 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0692\n",
      "Epoch 16 Batch 100 Loss 0.0796\n",
      "Epoch 16 Batch 200 Loss 0.0652\n",
      "Epoch 16 Batch 300 Loss 0.1053\n",
      "Epoch 16 Batch 400 Loss 0.0943\n",
      "Epoch 16 Loss 0.1004\n",
      "Time taken for 1 epoch 25.46847701072693 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0881\n",
      "Epoch 17 Batch 100 Loss 0.0663\n",
      "Epoch 17 Batch 200 Loss 0.0944\n",
      "Epoch 17 Batch 300 Loss 0.1044\n",
      "Epoch 17 Batch 400 Loss 0.1043\n",
      "Epoch 17 Loss 0.0976\n",
      "Time taken for 1 epoch 25.212384462356567 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0808\n",
      "Epoch 18 Batch 100 Loss 0.0700\n",
      "Epoch 18 Batch 200 Loss 0.1026\n",
      "Epoch 18 Batch 300 Loss 0.1130\n",
      "Epoch 18 Batch 400 Loss 0.0933\n",
      "Epoch 18 Loss 0.0879\n",
      "Time taken for 1 epoch 25.637207984924316 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0684\n",
      "Epoch 19 Batch 100 Loss 0.0633\n",
      "Epoch 19 Batch 200 Loss 0.0890\n",
      "Epoch 19 Batch 300 Loss 0.1209\n",
      "Epoch 19 Batch 400 Loss 0.0963\n",
      "Epoch 19 Loss 0.0797\n",
      "Time taken for 1 epoch 25.05808687210083 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0703\n",
      "Epoch 20 Batch 100 Loss 0.0803\n",
      "Epoch 20 Batch 200 Loss 0.0530\n",
      "Epoch 20 Batch 300 Loss 0.0611\n",
      "Epoch 20 Batch 400 Loss 0.1123\n",
      "Epoch 20 Loss 0.0769\n",
      "Time taken for 1 epoch 25.42178750038147 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1986744,
     "status": "ok",
     "timestamp": 1612535588224,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "H6kO2HY0bEn0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "en_word_index = {w:i for i,w in enumerate(en_vocab)}\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    \n",
    "    inputs = [en_word_index.get(w, 1) for w in sentence.split(' ')] # index 1 for [UNK]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([ja_vocab.index('<start>')], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input,\n",
    "                                          dec_hidden)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        if ja_vocab[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "        result += ja_vocab[predicted_id] + ' '\n",
    "\n",
    "        # 予測された ID がモデルに戻される\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q-zIxy2Zf3vw"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIbzsxI-f8Z9",
    "outputId": "36aa1e5e-5067-4db4-9fee-672ebe96beee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9838145ac8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir の中の最後のチェックポイントを復元\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "executionInfo": {
     "elapsed": 1987264,
     "status": "ok",
     "timestamp": 1612535588788,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjjMZX1uH0izIppjc9JsftJYiUHWDycpKYMG80UVQ=s64",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "aX4evgWVf9xa",
    "outputId": "831a9943-b7d0-4458-c854-0203c14ea490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i am a little out of sorts today . <end>\n",
      "Predicted translation: 今日 は 少し 気分 が 悪 い 。 \n"
     ]
    }
   ],
   "source": [
    "translate(\"i am a little out of sorts today .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9qYv1rYq1Lu5"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "\n",
    "input_sentences = load_data(os.path.join(data_dir, 'test.en'))\n",
    "target_sentences = load_data(os.path.join(data_dir, 'test.ja'))\n",
    "\n",
    "input_sentences = input_sentences[:TEST_SIZE_LIMIT]\n",
    "target_sentences = target_sentences[:TEST_SIZE_LIMIT]\n",
    "predicted_sentences = []\n",
    "\n",
    "for input_en in input_sentences:\n",
    "    predicted_ja, _ = evaluate(preprocess(input_en))\n",
    "    predicted_sentences.append(''.join(predicted_ja[:-1]))\n",
    "\n",
    "result_df = pd.DataFrame({'input_sentence': input_sentences,\n",
    "                          'target_sentence': target_sentences,\n",
    "                          'predicted_sentence': predicted_sentences})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "16qn07BeMCJs"
   },
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "for row in result_df.itertuples():\n",
    "    bleu_scores.append(\n",
    "        sentence_bleu(row.target_sentence, row.predicted_sentence,\n",
    "                      smoothing_function=SmoothingFunction().method4)\n",
    "    )\n",
    "result_df['bleu_score'] = bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "bGnwSEkf9HlM",
    "outputId": "9ed3d5f6-4579-49b9-c4e1-eee0f13b19c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "      <th>predicted_sentence</th>\n",
       "      <th>bleu_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they finally acknowledged it as true .</td>\n",
       "      <td>彼 ら は つい に それ が 真実 だ と 認め た 。</td>\n",
       "      <td>彼 ら は 彼 が 話 を する の を 許 し た 。</td>\n",
       "      <td>0.153911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he didn 't care for swimming .</td>\n",
       "      <td>彼 は 水泳 が 得意 で は な かっ た 。</td>\n",
       "      <td>彼 は 、 以前 、 彼 の こと に 賛成 し な かっ た 。</td>\n",
       "      <td>0.131693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he is no less kind than his sister .</td>\n",
       "      <td>彼 は お 姉 さん に 劣 ら ず 親切 だ 。</td>\n",
       "      <td>彼 は お 姉 さん ほど 背 が 高 く な い 。</td>\n",
       "      <td>0.166010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you must be back before ten .</td>\n",
       "      <td>１０ 時 前 に 戻 ら な けれ ば な ら な い 。</td>\n",
       "      <td>10 分 も か ら ず に つ か な けれ ば な ら な い 。</td>\n",
       "      <td>0.126820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>break a leg .</td>\n",
       "      <td>成功 を 祈 る わ 。</td>\n",
       "      <td>時 がいつ し か 過ぎ て 。</td>\n",
       "      <td>0.220057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>i 'm very sleepy now .</td>\n",
       "      <td>今 とても 眠 い 。</td>\n",
       "      <td>今 とても 忙し そう だ 。</td>\n",
       "      <td>0.313971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>above the music , i could hear her crying .</td>\n",
       "      <td>音楽 が な っ て い る の に 彼女 の 鳴き声 が 聞こえ た 。</td>\n",
       "      <td>私 は 音楽 を 聴 く ため に 彼女 の 夢 を 見 ま せ ん 。</td>\n",
       "      <td>0.122821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>we had the meeting in this room last friday .</td>\n",
       "      <td>先週 の 金曜 日 この 部屋 で 会合 が あ っ た 。</td>\n",
       "      <td>この バス で 私 たち は 昨日 、 交通 事故 に あ っ た 。</td>\n",
       "      <td>0.126820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>who do you want to speak to ?</td>\n",
       "      <td>お 話 に な る 方 の お 名前 は 。</td>\n",
       "      <td>あなた は 話 す べ き もの で は な い の か 。</td>\n",
       "      <td>0.136731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>his father eats there twice a week .</td>\n",
       "      <td>彼 の 父 は １ 週間 に ２ 回 そこ で 食べ る 。</td>\n",
       "      <td>父 が 一 週間 に 2、3 の 雨 が あ る ら しい 。</td>\n",
       "      <td>0.141483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    input_sentence  \\\n",
       "0           they finally acknowledged it as true .   \n",
       "1                   he didn 't care for swimming .   \n",
       "2             he is no less kind than his sister .   \n",
       "3                    you must be back before ten .   \n",
       "4                                    break a leg .   \n",
       "..                                             ...   \n",
       "295                         i 'm very sleepy now .   \n",
       "296    above the music , i could hear her crying .   \n",
       "297  we had the meeting in this room last friday .   \n",
       "298                  who do you want to speak to ?   \n",
       "299           his father eats there twice a week .   \n",
       "\n",
       "                           target_sentence  \\\n",
       "0            彼 ら は つい に それ が 真実 だ と 認め た 。   \n",
       "1                 彼 は 水泳 が 得意 で は な かっ た 。   \n",
       "2                彼 は お 姉 さん に 劣 ら ず 親切 だ 。   \n",
       "3            １０ 時 前 に 戻 ら な けれ ば な ら な い 。   \n",
       "4                             成功 を 祈 る わ 。   \n",
       "..                                     ...   \n",
       "295                            今 とても 眠 い 。   \n",
       "296  音楽 が な っ て い る の に 彼女 の 鳴き声 が 聞こえ た 。   \n",
       "297         先週 の 金曜 日 この 部屋 で 会合 が あ っ た 。   \n",
       "298                 お 話 に な る 方 の お 名前 は 。   \n",
       "299         彼 の 父 は １ 週間 に ２ 回 そこ で 食べ る 。   \n",
       "\n",
       "                       predicted_sentence  bleu_score  \n",
       "0            彼 ら は 彼 が 話 を する の を 許 し た 。    0.153911  \n",
       "1       彼 は 、 以前 、 彼 の こと に 賛成 し な かっ た 。    0.131693  \n",
       "2             彼 は お 姉 さん ほど 背 が 高 く な い 。    0.166010  \n",
       "3     10 分 も か ら ず に つ か な けれ ば な ら な い 。    0.126820  \n",
       "4                        時 がいつ し か 過ぎ て 。    0.220057  \n",
       "..                                    ...         ...  \n",
       "295                       今 とても 忙し そう だ 。    0.313971  \n",
       "296  私 は 音楽 を 聴 く ため に 彼女 の 夢 を 見 ま せ ん 。    0.122821  \n",
       "297   この バス で 私 たち は 昨日 、 交通 事故 に あ っ た 。    0.126820  \n",
       "298        あなた は 話 す べ き もの で は な い の か 。    0.136731  \n",
       "299       父 が 一 週間 に 2、3 の 雨 が あ る ら しい 。    0.141483  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9hQ_XKxIQ2W",
    "outputId": "3e30de47-f27d-48d9-9cb2-ddb562fbeb71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17331228745184793"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.bleu_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMVDvrdcVx8hEVViWJnjayE",
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": [
    {
     "file_id": "1DsM1Q65HCjdIloBDCnwGeO3xTxut581q",
     "timestamp": 1612529211758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
